In  |statistics|Statistics| , the likelihood principle is the proposition that, given a  |statistical_model|Statistical_Model| , all the evidence in a  |sample|Sampling|  relevant to model parameters is contained in the  |likelihood_function|Likelihood_Function| .     A likelihood function arises from a  |probability_density_function|Probability_Density_Function|  considered as a function of its distributional parameterization argument. For example, consider a model which gives the probability density function ƒ X  of observable  |random_variable|Random_Variable|  X as a function of a parameter  θ. Then for a specific value x of X, the function   L   ƒ X  is a likelihood function of  θθ is, if we know that X has the value  x. The density function may be a density with respect to counting measure, i.e. a  |probability_mass_function|Probability_Mass_Function| .     Two likelihood functions are equivalent if one is a scalar multiple of the other. The likelihood principle is this: all information from the data that is relevant to inferences about the value of the model parameters is in the equivalence class to which the likelihood function belongs. The strong likelihood principle applies this same criterion to cases such as sequential experiments where the sample of data that is available results from applying a  |stopping_rule|Stopping_Rule|  to the observations earlier in the experiment. Dodge, Y. The Oxford Dictionary of Statistical Terms. OUP.          Suppose     X is the number of successes in twelve  |independent|Statistical_Independence| |Bernoulli_trials|Bernoulli_Trial|  with probability θ of success on each trial, and   Y is the number of independent Bernoulli trials needed to get three successes, again with probability θ of success on each trial.     Then the observation that X 3 induces the likelihood function     :   L   3   220      while the observation that Y 12 induces the likelihood function     :   L   2   55      The likelihood principle says that, as the data are the same in both cases, the inferences drawn about the value of θ should also be the same. In addition, all the inferential content in the data about the value of θ is contained in the two likelihoods, and is the same if they are proportional to one another. This is the case in the above example, reflecting the fact that the difference between observing X 3 and observing Y 12 lies not in the actual data, but merely in the  |design_of_the_experiment|Design_Of_Experiments| . Specifically, in one case, one has decided in advance to try twelve times; in the other, to keep trying until three successes are observed. The inference about θ should be the same, and this is reflected in the fact that the two likelihoods are proportional to each other.     This is not always the case, however. The use of  |frequentist|Frequentist|  methods involving  |p-values|P-Values|  leads to different inferences for the two cases above,  showing that the outcome of frequentist methods depends on the experimental procedure, and thus violates the likelihood principle.         A related concept is the law of likelihood, the notion that the extent to which the evidence supports one parameter value or hypothesis against another is equal to the ratio of their likelihoods, their  |likelihood_ratio|Likelihood_Ratio| . That is,   :     is the degree to which the observation x supports parameter value or hypothesis a against b. If this ratio is 1, the evidence is indifferent; if greater than 1, the evidence supports the value a against b; or if less, then vice versa.     In  |Bayesian_statistics|Bayesian_Statistics| , this ratio is known as the  |Bayes_factor|Bayes_Factor| , and  |Bayes_rule|Bayes_Rule|  can be seen as the application of the law of likelihood to inference.     In  |frequentist_inference|Frequentist_Inference| , the likelihood ratio is used in the  |likelihood-ratio_test|Likelihood-Ratio_Test| , but other non-likelihood tests are used as well. The  |Neyman–Pearson_lemma|Neyman–Pearson_Lemma|  states the likelihood-ratio test is the most  |powerful|Statistical_Power|  test for comparing two  |simple_hypotheses|Simple_Hypothesis|  at a given  |significance_level|Significance_Level| , which gives a frequentist justification for the law of likelihood.     Combining the likelihood principle with the law of likelihood yields the consequence that the parameter value which maximizes the likelihood function is the value which is most strongly supported by the evidence. This is the basis for the widely used  |method_of_maximum_likelihood|Maximum_Likelihood| .         The likelihood principle was first identified by that name in print in 1962   ,   but arguments for the same principle, unnamed, and the use of the principle in applications goes back to the works of  |R.A._Fisher|Ronald_A._Fisher|  in the 1920s.   The law of likelihood was identified by that name by  |I._Hacking|Ian_Hacking|  .   More recently the likelihood principle as a general principle of inference has been championed by  |A._W._F._Edwards|A._W._F._Edwards| . The likelihood principle has been applied to the  |philosophy_of_science|Philosophy_Of_Science|  by R. Royall. Royall, Richard Statistical Evidence: A likelihood paradigm. Chapman and Hall, Boca Raton.       |Birnbaum|Allan_Birnbaum|  proved that the likelihood principle follows from two more primitive and seemingly reasonable principles, the  conditionality principle  and the  sufficiency principle . The conditionality principle says that if an experiment is chosen by a random process independent of the states of nature   , then only the experiment actually performed is relevant to inferences about   . The sufficiency principle says that if T is a  |sufficient_statistic|Sufficient_Statistic|  for   , and if in two experiments with data x1 and x2 we have TT , then the evidence about   given by the two experiments is the same.       Some widely used methods of conventional statistics, for example many  |significance_tests|Statistical_Hypothesis_Testing| , are not consistent with the likelihood principle.     Let us briefly consider some of the arguments for and against the likelihood principle.         Birnbaums proof of the likelihood principle has been disputed by philosophers of science, including  |Deborah_Mayo|Deborah_Mayo|  Mayo, D.  in Error and Inference: Recent Exchanges on Experimental Reasoning, Reliability and the Objectivity and Rationality of Science , Cambridge: Cambridge University Press: 305-314. Mayo, Deborah ,  ,  Statistical Science , 29: 227-266 . and statisticians including Michael Evans. Evans, Michael  On the other hand, a new proof of the likelihood principle has been provided by Greg Gandenberger. Gandenberger, Greg , A new proof of the likelihood principle ,  British Journal for the Philosophy of Science , 66: 475-503; .       Unrealized events play a role in some common statistical methods. For example, the result of a  |significance_test|Statistical_Hypothesis_Testing|  depends on the  |p-value|P-Value| , the probability of a result as extreme or more extreme than the observation, and that probability may depend on the design of the experiment. To the extent that the likelihood principle is accepted, such methods are therefore denied.     Some classical significance tests are not based on the likelihood. A commonly cited example is the  |optional_stopping|Optional_Stopping|  problem. Suppose I tell you that I tossed a coin 12 times and in the process observed 3 heads. You might make some inference about the probability of heads and whether the coin was fair. Suppose now I tell that I tossed the coin until I observed 3 heads, and I tossed it 12 times. Will you now make some different inference?     The likelihood function is the same in both cases: it is proportional to   : p3 9.     According to the likelihood principle, the inference should be the same in either case.     Suppose a number of scientists are assessing the probability of a certain outcome in experimental trials. Conventional wisdom suggests that if there is no bias towards success or failure then the success probability would be one half. Adam, a scientist, conducted 12 trials and obtains 3 successes and 9 failures. Then he left the lab.     Bill, a colleague in the same lab, continued Adams work and published Adams results, along with a significance test. He tested the  |null_hypothesis|Null_Hypothesis|  that p, the success probability, is equal to a half, versus p .