      In computing, interrupt latency is the time that elapses from when an  |interrupt|Interrupt|  is generated to when the source of the interrupt is serviced. For many operating systems, devices are serviced as soon as the devices  |interrupt_handler|Interrupt_Handler|  is executed. Interrupt latency may be affected by  |microprocessor|Microprocessor|  design,  |interrupt_controllers|Interrupt_Controller| ,  |interrupt_mask|Interrupt_Mask| ing, and the  |operating_systems|Operating_System|  interrupt handling methods.       There is usually a trade-off between interrupt latency,  |throughput|Throughput| , and processor utilization. Many of the techniques of  |CPU|Microprocessor|  and  |OS|Operating_System|  design that improve interrupt latency will decrease throughput and increase processor utilization. Techniques that increase throughput may increase interrupt latency and increase processor utilization. Lastly, trying to reduce processor utilization may increase interrupt latency and decrease throughput.     Minimum interrupt latency is largely determined by the  |interrupt_controller|Interrupt_Controller|  circuit and its configuration. They can also affect the  |jitter|Jitter|  in the interrupt latency, which can drastically affect the  |real-time|Real-Time_Computing| |schedulability|Scheduling|  of the system. The  |Intel_APIC_architecture|Intel_Apic_Architecture|  is well known for producing a huge amount of interrupt latency jitter.     Maximum interrupt latency is largely determined by the methods an OS uses for interrupt handling. For example, most processors allow programs to disable interrupts, putting off the execution of interrupt handlers, in order to protect  |critical_sections|Critical_Section|  of code. During the execution of such a critical section, all interrupt handlers that cannot execute safely within a critical section are blocked . So the interrupt latency for a blocked interrupt is extended to the end of the critical section, plus any interrupts with equal and higher priority that arrived while the block was in place.     Many computer systems require low interrupt latencies, especially  |embedded_systems|Embedded_System|  that need to  |control|Control_System|  machinery in real-time. Sometimes these systems use a  |real-time_operating_system|Real-Time_Operating_System|  . An RTOS makes the promise that no more than a specified maximum amount of time will pass between executions of  |subroutines|Subroutine| . In order to do this, the RTOS must also guarantee that interrupt latency will never exceed a predefined maximum.       Advanced interrupt controllers implement a multitude of hardware features in order to minimize the overhead during  |context_switch|Context_Switch| es and the effective interrupt latency. These include features like:     Minimum jitter through non-interruptible instructions   Zero wait states for the memory system   Switchable register banks   Tail chaining   Lazy stacking   Late arrival   Pop preemption   Sleep-on-exit feature     Also, there are many other methods hardware may use to help lower the requirements for shorter interrupt latency in order to make a given interrupt latency tolerable in a situation. These include buffers, and  |flow_control|Flow_Control| . For example, most network cards implement transmit and receive  |ring_buffers|Ring_Buffer| , interrupt rate limiting, and hardware flow control. Buffers allow data to be stored until it can be transferred, and flow control allows the network card to pause communications without having to discard data if the buffer is full.     Modern hardware also implements interrupt rate limiting. This helps prevent  |interrupt_storms|Interrupt_Storm|  or  |live-lock|Live-Lock|.