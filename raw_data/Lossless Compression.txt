Lossless compression is a class of  |data_compression|Data_Compression|  algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast,  |lossy_compression|Lossy_Compression|  permits reconstruction only of an approximation of the original data, though usually with greatly improved  |compression_rates|Bit_Ratebitrates_In_Multimedia|  .     Lossless data compression is used in many applications. For example, it is used in the  |ZIP|Zip|  file format and in the  |GNU|Gnu|  tool  |gzip|Gzip| . It is also often used as a component within lossy data compression technologies .     Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. Typical examples are executable programs, text documents, and source code. Some image file formats, like  |PNG|Portable_Network_Graphics|  or  |GIF|Graphics_Interchange_Format| , use only lossless compression, while others like  |TIFF|Tiff|  and  |MNG|Multiple_Image_Network_Graphics|  may use either lossless or lossy methods.  |Lossless_audio|Audio_Compression_Lossless|  formats are most often used for archiving or production purposes, while smaller  |lossy_audio|Audio_Compression_Lossy_Audio_Compression|  files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.       Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that probable data will produce shorter output than improbable data.     The primary encoding algorithms used to produce bit sequences are  |Huffman_coding|Huffman_Coding|   and  |arithmetic_coding|Arithmetic_Coding| . Arithmetic coding achieves compression rates close to the best possible for a particular statistical model, which is given by the  |information_entropy|Information_Entropy| , whereas Huffman compression is simpler and faster but produces poor results for models that deal with symbol probabilities close to 1.     There are two primary ways of constructing statistical models: in a static model, the data is analyzed and a model is constructed, then this model is stored with the compressed data. This approach is simple and modular, but has the disadvantage that the model itself can be expensive to store, and also that it forces using a single model for all data being compressed, and so performs poorly on files that contain heterogeneous data. Adaptive models dynamically update the model as the data is compressed. Both the encoder and decoder begin with a trivial model, yielding poor compression of initial data, but as they learn more about the data, performance improves. Most popular types of compression used in practice now use adaptive coders.     Lossless compression methods may be categorized according to the type of data they are designed to compress. While, in principle, any general-purpose lossless compression algorithm  can be used on any type of data, many are unable to achieve significant compression on data that are not of the form for which they were designed to compress. Many of the lossless compression techniques used for text also work reasonably well for  |indexed_images|Indexed_Color| .       These techniques take advantage of the specific characteristics of images such as the common phenomenon of contiguous 2-D areas of similar tones.   Every pixel but the first is replaced by the difference to its left neighbor. This leads to small values having a much higher probability than large values.   This is often also applied to sound files, and can compress files that contain mostly low frequencies and low volumes.   For images, this step can be repeated by taking the difference to the top pixel, and then in videos, the difference to the pixel in the next frame can be taken.     A hierarchical version of this technique takes neighboring pairs of data points, stores their difference and sum, and on a higher level with lower resolution continues with the sums. This is called  |discrete_wavelet_transform|Discrete_Wavelet_Transform| .  |JPEG2000|Jpeg2000|  additionally uses data points from other pairs and multiplication factors to mix them into the difference. These factors must be integers, so that the result is an integer under all circumstances. So the values are increased, increasing file size, but hopefully the distribution of values is more peaked.      The adaptive encoding uses the probabilities from the previous sample in sound encoding, from the left and upper pixel in image encoding, and additionally from the previous frame in video encoding. In the wavelet transformation, the probabilities are also passed through the hierarchy.       Many of these methods are implemented in open-source and proprietary tools, particularly LZW and its variants. Some algorithms are patented in the  |United_States|United_States|  and other countries and their legal usage requires licensing by the patent holder. Because of patents on certain kinds of LZW compression, and in particular licensing practices by patent holder Unisys that many developers considered abusive, some open source proponents encouraged people to avoid using the  |Graphics_Interchange_Format|Graphics_Interchange_Format|  for compressing still image files in favor of  |Portable_Network_Graphics|Portable_Network_Graphics|  , which combines the  |LZ77|Lz77_And_Lz78| -based  |deflate|Deflate|  algorithm with a selection of domain-specific prediction filters. However, the patents on LZW expired on June 20, 2003.      Many of the lossless compression techniques used for text also work reasonably well for  |indexed_images|Indexed_Image| , but there are other techniques that do not work for typical text that are useful for some images , and other techniques that take advantage of the specific characteristics of images .     As mentioned previously, lossless sound compression is a somewhat specialized area. Lossless sound compression algorithms can take advantage of the repeating patterns shown by the wave-like nature of the data – essentially using  |autoregressive|Autoregressive|  models to predict the next value and encoding the difference between the expected value and the actual data. If the difference between the predicted and the actual data  tends to be small, then certain difference values become very frequent, which can be exploited by encoding them in few output bits.     It is sometimes beneficial to compress only the differences between two versions of a file . This is called  |delta_encoding|Delta_Encoding|  , but the term is typically only used if both versions are meaningful outside compression and decompression. For example, while the process of compressing the error in the above-mentioned lossless audio compression scheme could be described as delta encoding from the approximated sound wave to the original sound wave, the approximated version of the sound wave is not meaningful in any other context.            By operation of the  |pigeonhole_principle|Pigeonhole_Principle| , no lossless compression algorithm can efficiently compress all possible data. For this reason, many different algorithms exist that are designed either with a specific type of input data in mind or with specific assumptions about what kinds of redundancy the uncompressed data are likely to contain.     Some of the most common lossless compression algorithms are listed below.        |bzip2|Bzip2|  – Combines  |Burrows–Wheeler_transform|Burrows–Wheeler_Transform|  with RLE and Huffman coding    |Finite_State_Entropy|Finite_State_Entropy|  –  |Entropy_encoding|Entropy_Encoding| , a tabled variant of  |ANS|Asymmetric_Numeral_Systems| , used by LZFSE and Zstandard    |Huffman_coding|Huffman_Coding|  – Entropy encoding, pairs well with other algorithms, used by Unixs  |pack|Pack|  utility    |Lempel_Ziv_compression|Lz77_And_Lz78|  – Dictionary-based algorithm that forms the basis for many other algorithms    |Lempel–Ziv–Markov_chain_algorithm|Lempel–Ziv–Markov_Chain_Algorithm|  – Very high compression ratio, used by  |7zip|7Zip|  and  |xz|Xz_Utils| |_Lempel–Ziv–Oberhumer|Lempel–Ziv–Oberhumer|  – Designed for speed at the expense of compression ratios    |Lempel–Ziv–Storer–Szymanski|Lempel–Ziv–Storer–Szymanski|  – Used by  |WinRAR|Winrar|  in tandem with Huffman coding    |DEFLATE|Deflate|  – Combines LZSS compression with Huffman coding, used by  |ZIP|Zip| ,  |gzip|Gzip| , and  |PNG|Portable_Network_Graphics|  images    |Lempel–Ziv–Welch|Lempel–Ziv–Welch|  – Used by  |GIF|Gif|  images and Unixs  |compress|Compress|  utility    |Lempel–Ziv_Finite_State_Entropy|Lzfse|  – Combines  |Lempel–Ziv|Lempel–Ziv|  and Finite State Entropy, used by  |iOS|Ios|  and  |macOS|Macos| |_Zstandard|Zstandard|  – Combines LZ77, Finite State Entropy, and Huffman coding, used by the  |Linux_kernel|Linux_Kernel| |_Prediction_by_partial_matching|Prediction_By_Partial_Matching|  – Optimized for compressing  |plain_text|Plain_Text| |_Run_length_encoding|Run_Length_Encoding|  – Simple scheme that provides good compression of data containing lots of runs of the same value        |Apple_Lossless|Apple_Lossless| |_Adaptive_Transform_Acoustic_Coding|Adaptive_Transform_Acoustic_Coding| |_Audio_Lossless_Coding|Audio_Lossless_Coding| |_Direct_Stream_Transfer|Super_Audio_Cddst| |_Dolby_TrueHD|Dolby_Truehd| |_DTS_HD_Master_Audio|Dts_Hd_Master_Audio| |_Free_Lossless_Audio_Codec|Free_Lossless_Audio_Codec| |_Meridian_Lossless_Packing|Meridian_Lossless_Packing| |_Monkeys_Audio|Monkeys_Audio| |_MPEG_4_SLS|Mpeg_4_Sls| |_OptimFROG|Optimfrog| |_Original_Sound_Quality|Original_Sound_Quality| |_RealPlayer|Realplayer| |_Shorten|Shorten| |_TTA|Tta| |_WavPack|Wavpack| |_WMA_Lossless|Windows_Media_Audio_9_Lossless| |_HEIF|High_Efficiency_Image_File_Format|  – High Efficiency Image File Format     |ILBM|Ilbm|  –     |LDCT|Discrete_Cosine_Transform|  – Lossless  |Discrete_Cosine_Transform|Discrete_Cosine_Transform| |_JBIG2|Jbig2|  –    |JPEG_2000|Jpeg_2000|  –     |JPEG_XR|Jpeg_Xr|  – formerly WMPhoto and HD Photo, includes a lossless compression method    |JPEG_LS|Lossless_Jpegjpeg_Ls|  –    |PCX|Pcx|  – PiCture eXchange    |PDF|Portable_Document_Format|  – Portable Document Format    |PNG|Portable_Network_Graphics|  – Portable Network Graphics    |TIFF|Tagged_Image_File_Format|  – Tagged Image File Format    |TGA|Truevision_Tga|  – Truevision TGA    |WebP|Webp|  –    |FLIF|Flif|  – Free Lossless Image Format        |OpenCTM|Openctm|  – Lossless compression of 3D triangle meshes         See  |this_list|List_Of_Codecslossless_Video_Compression|  of lossless video codecs.        |Cryptosystems|Cryptosystem|  often compress data before encryption for added security. When properly implemented, compression greatly increases the  |unicity_distance|Unicity_Distance|  by removing patterns that might facilitate  |cryptanalysis|Cryptanalysis| .  However, many ordinary lossless compression algorithms produce headers, wrappers, tables, or other predictable output that might instead make cryptanalysis easier. Thus, cryptosystems must utilize compression algorithms whose output does not contain these predictable patterns.       Genetics compression algorithms  are the latest generation of lossless algorithms that compress data using both conventional compression algorithms and specific algorithms adapted to genetic data. In 2012, a team of scientists from Johns Hopkins University published the first genetic compression algorithm that does not rely on external genetic databases for compression. HAPZIPPER was tailored for  |HapMap|Internationalhapmapproject|  data and achieves over 20-fold compression , providing 2- to 4-fold better compression much faster than leading general-purpose compression utilities.      Genomic sequence compression algorithms, also known as DNA sequence compressors, explore the fact that DNA sequences have characteristic properties, such as inverted repeats. The most successful compressors are XM and GeCo.  For  |eukaryotes|Eukaryotes|  XM is slightly better in compression ratio, though for sequences larger than 100 MB its computational requirements are impractical.          Self-extracting executables contain a compressed application and a decompressor. When executed, the decompressor transparently decompresses and runs the original application. This is especially often used in  |demo|Demo|  coding, where competitions are held for demos with strict size limits, as small as  |1k|Kilobyte| .   This type of compression is not strictly limited to binary executables, but can also be applied to scripts, such as  |JavaScript|Javascript| .       Lossless compression algorithms and their implementations are routinely tested in head-to-head  |benchmarks|Benchmark| . There are a number of better-known compression benchmarks. Some benchmarks cover only the  |data_compression_ratio|Data_Compression_Ratio| , so winners in these benchmarks may be unsuitable for everyday use due to the slow speed of the top performers. Another drawback of some benchmarks is that their data files are known, so some program writers may optimize their programs for best performance on a particular data set. The winners on these benchmarks often come from the class of  |context_mixing|Context_Mixing|  compression software.      |Matt_Mahoney|Matt_Mahoney| , in his February 2010 edition of the free booklet Data Compression Explained, additionally lists the following:    The  |Calgary_Corpus|Calgary_Corpus|  dating back to 1987 is no longer widely used due to its small size. Matt Mahoney currently maintains the Calgary Compression Challenge, created and maintained from May 21, 1996 through May 21, 2016 by Leonid A. Broukhis.   The Large Text Compression Benchmark  and the similar  |Hutter_Prize|Hutter_Prize|  both use a trimmed  |Wikipedia|Wikipedia| |_XML|Xml| |_UTF_8|Utf_8|  data set.   The Generic Compression Benchmark  , maintained by Mahoney himself, test compression on random data.   Sami Runsas maintains Compression Ratings, a benchmark similar to Maximum Compression multiple file test, but with minimum speed requirements. It also offers a calculator that allows the user to weight the importance of speed and compression ratio. The top programs here are fairly different due to speed requirement. In January 2010, the top programs were NanoZip followed by  |FreeArc|Freearc| ,  |CCM|Ccm| ,  |flashzip|Flashzip| , and  |7_Zip|7_Zip| .   The Monster of Compression benchmark by N. F. Antonio tests compression on 1Gb of public data with a 40-minute time limit. As of Dec. 20, 2009 the top ranked archiver is NanoZip 0.07a and the top ranked single file compressor is  |ccmx|Ccmx|  1.30c, both  |context_mixing|Context_Mixing| .     The Compression Ratings website published a chart summary of the frontier in compression ratio and time.      The Compression Analysis Tool  is a Windows application that enables end users to benchmark the performance characteristics of streaming implementations of LZF4, DEFLATE, ZLIB, GZIP, BZIP2 and LZMA using their own data. It produces measurements and charts with which users can compare the compression speed, decompression speed and compression ratio of the different compression methods and to examine how the compression level, buffer size and flushing operations affect the results.       Lossless data compression algorithms cannot guarantee compression for all input data sets. In other words, for any lossless data compression algorithm, there will be an input data set that does not get smaller when processed by the algorithm, and for any lossless data compression algorithm that makes at least one file smaller, there will be at least one file that it makes larger. This is easily proven with elementary mathematics using a  |counting_argument|Counting_Argument| , as follows:     Assume that each file is represented as a string of bits of some arbitrary length.   Suppose that there is a compression algorithm that transforms every file into an output file that is no longer than the original file, and that at least one file will be compressed into an output file that is shorter than the original file.   Let M be the least number such that there is a file F with length M bits that compresses to something shorter. Let N be the length of the compressed version of F.   Because N  .