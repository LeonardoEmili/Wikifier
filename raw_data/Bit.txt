           The bit is a  |basic_unit|Units_Of_Information|  of  |information|Information|  in  |information_theory|Information_Theory| ,  |computing|Computing| , and digital  |communications|Communication| . The name is a  |portmanteau|Portmanteau|  of binary digit.     In  |information_theory|Information_Theory| , one bit is typically defined as the  |information_entropy|Information_Entropy|  of a binary random variable that is 0 or 1 with equal probability, or the information that is gained when the value of such a variable becomes known. As a  |unit_of_information|Unit_Of_Information| , the bit has also been called a  shannon , named after  |Claude_E_Shannon|Claude_E_Shannon| .     As a  |binary|Binary_Number|  digit, the bit represents a  |logical_value|Truth_Value| , having only one of two  |values|Value| . It may be physically implemented with a two-state device. These state values are most commonly represented as either , but other representations such as true/false, yes/no, +/−, or on/off are possible. The correspondence between these values and the physical states of the underlying  |storage|Data_Storage_Device|  or  |device|Computing_Device|  is a matter of convention, and different assignments may be used even within the same device or  |program|Computer_Program| .     The symbol for the binary digit is either simply bit per recommendation by the  |IEC_80000_13|Iec_80000_13| b, as recommended by the  |IEEE_1541_2002|Ieee_1541_2002|  and  |IEEE_Std_260_1_2004|Ieee_Std_260_1_2004|  standards. A group of eight binary digits is commonly called one   |byte|Byte| , but historically the size of the byte is not strictly defined.       The encoding of data by discrete bits was used in the  |punched_cards|Punched_Card|  invented by  |Basile_Bouchon|Basile_Bouchon|  and Jean-Baptiste Falcon , developed by  |Joseph_Marie_Jacquard|Joseph_Marie_Jacquard|  , and later adopted by  |Semyon_Korsakov|Semyon_Korsakov| ,  |Charles_Babbage|Charles_Babbage| ,  |Hermann_Hollerith|Hermann_Hollerith| , and early computer manufacturers like  |IBM|Ibm| . Another variant of that idea was the perforated  |paper_tape|Paper_Tape| . In all those systems, the medium conceptually carried an array of hole positions; each position could be either punched through or not, thus carrying one  bit of information. The encoding of text by bits was also used in  |Morse_code|Morse_Code|  and early digital communications machines such as  |teletypes|Teleprinter|  and  |stock_ticker_machines|Stock_Ticker_Machine|  .      |Ralph_Hartley|Ralph_Hartley|  suggested the use of a logarithmic measure of information in 1928. Claude E. Shannon first used the word bit in his seminal 1948 paper  |A_Mathematical_Theory_of_Communication|A_Mathematical_Theory_Of_Communication|  . He attributed its origin to  |John_W_Tukey|John_W_Tukey| , who had written a Bell Labs memo on 9 January 1947 in which he contracted binary information digit to simply bit .  |Vannevar_Bush|Vannevar_Bush|  had written in 1936 of bits of information that could be stored on the  |punched_cards|Punched_Card|  used in the mechanical computers of that time. The first programmable computer, built by  |Konrad_Zuse|Konrad_Zuse| , used binary notation for numbers.      |Flip_flop_|Flip_Flop|  --  ref name NIST2008 /  ref name Bemer2000 / ref name Buchholz1956 / ref name Buchholz1977 / ref name Buchholz1962 / ref name Bemer1959 /  ref name Information in small bits /  n  bits of information, then that information can in principle be encoded in about m  bits, at least on the average. This principle is the basis of  |data_compression|Lossless_Data_Compression|  technology. Using an analogy, the hardware binary digits refer to the amount of storage space available , and the information content the filling, which comes in different levels of granularity . When the granularity is finer—when information is more compressed—the same bucket can hold more.     For example, it is estimated that the combined technological capacity of the world to store information provides 1,300  |exabytes|Exabytes|  of hardware digits in 2007. However, when this storage space is filled and the corresponding content is optimally compressed, this only represents 295  |exabytes|Exabytes|  of information. When optimally compressed, the resulting carrying capacity approaches  |Shannon_information|Shannon_Information|  or  |information_entropy|Information_Entropy| .       Certain  |bitwise|Bitwise_Operation|  computer  |processor|Central_Processing_Unit|  instructions  operate at the level of manipulating bits rather than manipulating data interpreted as an aggregate of bits.     In the 1980s, when  |bitmap|Bitmap| ped computer displays became popular, some computers provided specialized  |bit_block_transfer|Bitblt|  instructions to set or copy the bits that corresponded to a given rectangular area on the screen.     In most computers and programming languages, when a bit within a group of bits, such as a byte or word, is referred to, it is usually specified by a number from 0 upwards corresponding to its position within the byte or word. However, 0 can refer to either the  |most|Most_Significant_Bit|  or  |least_significant_bit|Least_Significant_Bit|  depending on the context.          Similar to  |angular_momentum|Angular_Momentum|  and  |energy|Energy|  in physics;  |information_theoretic_information|Informationinformation_Theory_Approach|  and data storage size have the same  |dimensionality|Dimensional_Analysis|  of  |units_of_measurement|Unit_Of_Measurement| , but there is in general no meaning to adding, subtracting or otherwise combining the units mathematically.     Other units of information, sometimes used in information theory, include the  natural digit  also called a  nat  or  nit  and defined as  |log|Logarithm|  2  e  bits, where e is the  |base_of_the_natural_logarithms|E| ; and the  dit ,  ban , or  hartley , defined as log 2  10  bits. This value, slightly less than 10/3, may be understood because 10 3 1000 ≈ 1024 2 10 : three decimal digits are slightly less information than ten binary digits, so one decimal digit is slightly less than 10/3 binary digits. Conversely, one  bit of information corresponds to about  |ln|Natural_Logarithm|   2 nats, or log 10  2 hartleys. As with the inverse ratio, this value, approximately 3/10, but slightly more, corresponds to the fact that 2 10 1024 ~ 1000 10 3 : ten binary digits are slightly more information than three decimal digits, so one binary digit is slightly more than 3/10 decimal digits. Some authors also define a binit .