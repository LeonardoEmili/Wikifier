In  |cryptography|Cryptography| , key size or key length is the number of  |bits|Bit|  in a  |key|Key|  used by a  |cryptographic|Cryptography|  algorithm .     Key length defines the upper-bound on an algorithms  |security|Password_Strength|  , since the security of all algorithms can be violated by  |brute-force_attacks|Brute-Force_Attack| . Ideally, key length would coincide with the lower-bound on an algorithms security. Indeed, most  |symmetric-key_algorithms|Symmetric-Key_Algorithm|  are designed to have security equal to their key length. However, after design, a new attack might be discovered. For instance,  |Triple_DES|Triple_Des|  was designed to have a 168 bit key, but an attack of complexity 2 112 is now known . Nevertheless, as long as the relation between key length and security is sufficient for a particular application, then it doesnt matter if key length and security coincide. This is important for  |asymmetric-key_algorithms|Asymmetric-Key_Algorithm| , because no such algorithm is known to satisfy this property;  |elliptic_curve_cryptography|Elliptic_Curve_Cryptography|  comes the closest with an effective security of roughly half its key length.        |Keys|Key|  are used to control the operation of a cipher so that only the correct key can convert encrypted text  to  |plaintext|Plaintext| . Many ciphers are actually based on publicly known  |algorithms|Algorithm|  or are  |open_source|Open-Source_Model|  and so it is only the difficulty of obtaining the key that determines security of the system, provided that there is no analytic attack , and assuming that the key is not otherwise available . The widely accepted notion that the security of the system should depend on the key alone has been explicitly formulated by  |Auguste_Kerckhoffs|Auguste_Kerckhoffs|  and  |Claude_Shannon|Claude_Shannon|  ; the statements are known as  |Kerckhoffs_principle|Kerckhoffs_Principle|  and Shannons Maxim respectively.     A key should therefore be large enough that a brute-force attack is infeasible   i.e. would take too long to execute.  |Shannons|Claude_Shannon|  work on  |information_theory|Information_Theory|  showed that to achieve so called  perfect secrecy , the key length must be at least as large as the message and only used once . In light of this, and the practical difficulty of managing such long keys, modern cryptographic practice has discarded the notion of perfect secrecy as a requirement for encryption, and instead focuses on  computational security , under which the computational requirements of breaking an encrypted text must be infeasible for an attacker.       Encryption systems are often grouped into families. Common families include symmetric systems  and asymmetric systems ; they may alternatively be grouped according to the central  |algorithm|Algorithm|  used .     As each of these is of a different level of cryptographic complexity, it is usual to have different key sizes for the same  |level_of_security|Level_Of_Security| , depending upon the algorithm used. For example, the security available with a 1024-bit key using asymmetric  |RSA|Rsa|  is considered approximately equal in security to an 80-bit key in a symmetric algorithm.      The actual degree of security achieved over time varies, as more computational power and more powerful mathematical analytic methods become available. For this reason cryptologists tend to look at indicators that an algorithm or key length shows signs of potential vulnerability, to move to longer key sizes or more difficult algorithms. For example, , a 1039 bit integer was factored with the  |special_number_field_sieve|Special_Number_Field_Sieve|  using 400 computers over 11 months.  The factored number was of a special form; the special number field sieve cannot be used on RSA keys. The computation is roughly equivalent to breaking a 700 bit RSA key. However, this might be an advance warning that 1024 bit RSA used in secure online commerce should be  |deprecated|Deprecation| , since they may become breakable in the near future. Cryptography professor  |Arjen_Lenstra|Arjen_Lenstra|  observed that Last time, it took nine years for us to generalize from a special to a nonspecial, hard-to-factor number and when asked whether 1024-bit RSA keys are dead, said: The answer to that question is an unqualified yes.      The 2015  |Logjam_attack|Logjam|  revealed additional dangers in using Diffie-Helman key exchange when only one or a few common 1024-bit or smaller prime moduli are in use. This common practice allows large amounts of communications to be compromised at the expense of attacking a small number of primes.                Even if a symmetric cipher is currently unbreakable by exploiting structural weaknesses in its algorithm, it is possible to run through the entire  |space|Space|  of keys in what is known as a brute-force attack. Since longer symmetric keys require exponentially more work to brute force search, a sufficiently long symmetric key makes this line of attack impractical.     With a key of length n bits, there are 2 n possible keys. This number grows very rapidly as n increases. The large number of operations required to try all possible 128-bit keys is widely considered  |out_of_reach|Large_Numberscomputers_And_Computational_Complexity|  for conventional digital computing techniques for the foreseeable future.  However, experts anticipate alternative computing technologies that may have processing power superior to current computer technology. If a suitably sized  |quantum_computer|Quantum_Computer|  capable of running  |Grovers_algorithm|Grovers_Algorithm|  reliably becomes available, it would reduce a 128-bit key down to 64-bit security, roughly a  |DES|Data_Encryption_Standard|  equivalent. This is one of the reasons why  |AES|Advanced_Encryption_Standard|  supports a 256-bit key length. See the discussion on the relationship between key lengths and quantum computing attacks at the bottom of this page for more information.       US Government export policy has long  |restricted_the_strength_of_cryptography|Export_Of_Cryptography_In_The_United_States|  that can be sent out of the country. For many years the limit was  |40_bits|40-Bit_Encryption| . Today, a key length of 40 bits offers little protection against even a casual attacker with a single PC, a predictable and inevitable consequence of governmental restrictions limiting key length. In response, by the year 2000, most of the major US restrictions on the use of strong encryption were relaxed.  However, not all regulations have been removed, and encryption registration with the  |U.S._Bureau_of_Industry_and_Security|U.S._Bureau_Of_Industry_And_Security|  is still required to export mass market encryption commodities, software and components with encryption exceeding 64 bits .     IBMs  |Lucifer_cipher|Lucifer|  was selected in 1974 as the base for what would become the  |Data_Encryption_Standard|Data_Encryption_Standard| . Lucifers key length was reduced from 128 bits to  |56_bits|56-Bit_Encryption| , which the  |NSA|National_Security_Agency|  and NIST argued was sufficient. The NSA has major computing resources and a large budget; some cryptographers including  |Whitfield_Diffie|Whitfield_Diffie|  and  |Martin_Hellman|Martin_Hellman|  complained that this made the cipher so weak that NSA computers would be able to break a DES key in a day through brute force parallel computing. The NSA disputed this, claiming brute forcing DES would take them something like 91 years.  However, by the late 90s, it became clear that DES could be cracked in a few days time-frame with custom-built hardware such as could be purchased by a large corporation or government.  , Cato Institute Briefing Paper no. 51, Arnold G. Reinhold, 1999 The book Cracking DES tells of the successful attempt in 1998 to break 56-bit DES by a brute-force attack mounted by a cyber civil rights group with limited resources; see  |EFF_DES_cracker|Eff_Des_Cracker| . Even before that demonstration, 56 bits was considered insufficient length for  |symmetric_algorithm|Symmetric-Key_Algorithm|  keys; DES has been replaced in many applications by  |Triple_DES|Triple_Des| , which has 112 bits of security when used 168-bit keys . In 2002,  |Distributed.net|Distributed.Net|  and its volunteers broke a 64-bit RC5 key after several years effort, using about seventy thousand computers.     The  |Advanced_Encryption_Standard|Advanced_Encryption_Standard|  published in 2001 uses key sizes of 128 bits, 192 or 256 bits. Many observers consider 128 bits sufficient for the foreseeable future for symmetric algorithms of  |AESs|Advanced_Encryption_Standard|  quality until  |quantum_computers|Quantum_Computer|  become available. However, as of 2015, the U.S.  |National_Security_Agency|National_Security_Agency|  has issued guidance that it plans to switch to quantum computing resistant algorithms and now requires 256-bit  |AES|Advanced_Encryption_Standard|  keys for data  |classified_up_to_Top_Secret|Classified_Information_In_The_United_States| .      In 2003, the U.S. National Institute for Standards and Technology,  |NIST|Nist|  proposed phasing out 80-bit keys by 2015. At 2005, 80-bit keys were allowed only until 2010.   Special Publication 800-57 Recommendation for Key Management â€“ Part 1: General, original version 2005, Table 4, Csrc.nist.gov     Since 2015, NIST guidance says that the use of keys that provide less than 112 bits of security strength for key agreement is now disallowed. NIST approved symmetric encryption algorithms include three-key  |Triple_DES|Triple_Des| , and  |AES|Advanced_Encryption_Standard| . Approvals for two-key Triple DES and  |Skipjack|Skipjack|  were withdrawn in 2015; the  |NSAs|Nsa|  Skipjack algorithm used in its  |Fortezza|Fortezza|  program employs 80-bit keys.        The effectiveness of  |public_key_cryptosystems|Public_Key_Cryptography|  depends on the intractability of certain mathematical problems such as  |integer_factorization|Integer_Factorization| . These problems are time consuming to solve, but usually faster than trying all possible keys by brute force. Thus, asymmetric algorithm keys must be longer for equivalent resistance to attack than symmetric algorithm keys. As of 2002, an  |asymmetric_key|Asymmetric_Key|  length of 1024 bits was generally considered by cryptology experts to be the minimum necessary for the  |RSA|Rsa|  encryption algorithm.        |RSA_Security|Rsa_Security|  claims that 1024-bit RSA keys are equivalent in strength to 80-bit symmetric keys, 2048-bit RSA keys to 112-bit symmetric keys and 3072-bit RSA keys to 128-bit symmetric keys.  RSA claims that 1024-bit keys are likely to become crackable some time between 2006 and 2010 and that 2048-bit keys are sufficient until 2030. The NIST recommends 2048-bit keys for RSA.  An RSA key length of 3072 bits should be used if security is required beyond 2030.  NIST key management guidelines further suggest that 15360-bit RSA keys are equivalent in strength to 256-bit symmetric keys.      The Finite Field  |Diffie-Hellman|Diffie-Hellman|  algorithm has roughly the same key strength as RSA for the same key sizes. The work factor for breaking Diffie-Hellman is based on the  |discrete_logarithm_problem|Discrete_Logarithm_Problem| , which is related to the integer factorization problem on which RSAs strength is based. Thus, a 3072-bit Diffie-Hellman key has about the same strength as a 3072-bit RSA key.     One of the asymmetric algorithm types,  |elliptic_curve_cryptography|Elliptic_Curve_Cryptography| , or ECC, appears to be secure with shorter keys than other asymmetric key algorithms require.  |NIST|Nist|  guidelines state that ECC keys should be twice the length of equivalent strength symmetric key algorithms. So, for example, a 224-bit ECC key would have roughly the same strength as a 112-bit symmetric key. These estimates assume no major breakthroughs in solving the underlying mathematical problems that ECC is based on. A message encrypted with an elliptic key algorithm using a 109-bit long key has been broken by brute force.      The  |NSA|Nsa|  previously specified that Elliptic Curve Public Key Cryptography using the 256-bit prime modulus elliptic curve as specified in FIPS-186-2 and SHA-256 are appropriate for protecting classified information up to the SECRET level. Use of the 384-bit prime modulus elliptic curve and SHA-384 are necessary for the protection of TOP SECRET information.  In 2015 the NSA announced that it plans to transition from Elliptic Curve Cryptography to new algorithms that are resistant to attack by future  |quantum_computers|Quantum_Computer| . In the interim it recommends the larger  |384-bit_curve|P-384|  for all classified information.       The two best known quantum computing attacks are based on  |Shors_algorithm|Shors_Algorithm|  and  |Grovers_algorithm|Grovers_Algorithm| . Of the two, Shors offers the greater risk to current security systems.     Derivatives of Shors algorithm are widely conjectured to be effective against all mainstream public-key algorithms including  |RSA|Rsa| ,  |Diffie-Hellman|Diffie-Hellman|  and  |elliptic_curve_cryptography|Elliptic_Curve_Cryptography| . According to Professor Gilles Brassard, an expert in quantum computing: The time needed to factor an RSA integer is the same order as the time needed to use that same integer as modulus for a single RSA encryption. In other words, it takes no more time to break RSA on a quantum computer than to use it legitimately on a classical computer. The general consensus is that these public key algorithms are insecure at any key size if sufficiently large quantum computers capable of running Shors algorithm become available. The implication of this attack is that all data encrypted using current standards based security systems such as the ubiquitous  |SSL|Transport_Layer_Security|  used to protect e-commerce and Internet banking and  |SSH|Secure_Shell|  used to protect access to sensitive computing systems is at risk. Encrypted data protected using public-key algorithms can be archived and may be broken at a later time.     Mainstream symmetric ciphers  and collision resistant hash functions  are widely conjectured to offer greater security against known quantum computing attacks. They are widely thought most vulnerable to  |Grovers_algorithm|Grovers_Algorithm| . Bennett, Bernstein, Brassard, and Vazirani proved in 1996 that a brute-force key search on a quantum computer cannot be faster than roughly 2 n/2 invocations of the underlying cryptographic algorithm, compared with roughly 2 n in the classical case. Bennett C.H., Bernstein E., Brassard G., Vazirani U., http://www.cs.berkeley.edu/~vazirani/pubs/bbbv.ps The strengths and weaknesses of quantum computation.  |SIAM_Journal_on_Computing|Siam_Journal_On_Computing|  26: 1510-1523 . Thus in the presence of large quantum computers an n-bit key can provide at least n/2 bits of security. Quantum brute force is easily defeated by doubling the key length, which has little extra computational cost in ordinary use. This implies that at least a 256-bit symmetric key is required to achieve 128-bit security rating against a quantum computer. As mentioned above, the NSA announced in 2015 that it plans to transition to quantum-resistant algorithms.     According to NSA A sufficiently large quantum computer, if built, would be capable of undermining all widely-deployed public key algorithms used for key establishment and digital signatures. ... It is generally accepted that quantum computing techniques are much less effective against symmetric algorithms than against current widely used public key algorithms. While public key cryptography requires changes in the fundamental design to protect against a potential future quantum computer, symmetric key algorithms are believed to be secure provided a sufficiently large key size is used. ... In the longer term, NSA looks to  |NIST|National_Institute_Of_Standards_And_Technology|  to identify a broadly accepted, standardized suite of commercial public key algorithms that are not vulnerable to quantum attacks.     , the NSAs Commercial National Security Algorithm Suite includes:  U.S. National Security Agency, January 2016      .