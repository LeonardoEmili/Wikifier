In  |statistics|Statistics| , the Kolmogorov–Smirnov test  is a  |nonparametric_test|Nonparametric_Statistics|  of the equality of continuous , one-dimensional  |probability_distributions|Probability_Distribution|  that can be used to compare a  |sample|Random_Sample|  with a reference probability distribution , or to compare two samples . It is named after  |Andrey_Kolmogorov|Andrey_Kolmogorov|  and  |Nikolai_Smirnov|Nikolai_Smirnov| .     The Kolmogorov–Smirnov statistic quantifies a  |distance|Metric|  between the  |empirical_distribution_function|Empirical_Distribution_Function|  of the sample and the  |cumulative_distribution_function|Cumulative_Distribution_Function|  of the reference distribution, or between the empirical distribution functions of two samples. The  |null_distribution|Null_Distribution|  of this statistic is calculated under the  |null_hypothesis|Null_Hypothesis|  that the sample is drawn from the reference distribution or that the samples are drawn from the same distribution . In the one-sample case, the distribution considered under the null hypothesis may be continuous , purely discrete or mixed . In the two-sample case , the distribution considered under the null hypothesis is a continuous distribution but is otherwise unrestricted.     The two-sample K–S test is one of the most useful and general nonparametric methods for comparing two samples, as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples.     The Kolmogorov–Smirnov test can be modified to serve as a  |goodness_of_fit|Goodness_Of_Fit|  test. In the special case of testing for  |normality|Normal_Distribution|  of the distribution, samples are standardized and compared with a standard normal distribution. This is equivalent to setting the mean and variance of the reference distribution equal to the sample estimates, and it is known that using these to define the specific reference distribution changes the null distribution of the test statistic . Various studies have found that, even in this corrected form, the test is less powerful for testing normality than the  |Shapiro–Wilk_test|Shapiro–Wilk_Test|  or  |Anderson–Darling_test|Anderson–Darling_Test| .  However, these other tests have their own disadvantages. For instance the Shapiro–Wilk test is known not to work well in samples with many identical values.       The  |empirical_distribution_function|Empirical_Distribution_Function|  F n for n  |independent_and_identically_distributed|Independent_And_Identically_Distributed_Random_Variables|  ordered observations X i  is defined as     : Fn  I     where I is the  |indicator_function|Indicator_Function| , equal to 1 if Xi   x and equal to 0 otherwise.     The Kolmogorov–Smirnov  |statistic|Statistic|  for a given  |cumulative_distribution_function|Cumulative_Distribution_Function|  F is     : Dn    Fn-F      where sup x is the  |supremum|Supremum|  of the set of distances. By the  |Glivenko–Cantelli_theorem|Glivenko–Cantelli_Theorem| , if the sample comes from distribution F, then D n converges to 0  |almost_surely|Almost_Surely|  in the limit when n goes to infinity. Kolmogorov strengthened this result, by effectively providing the rate of this convergence .  |Donskers_theorem|Donskers_Theorem|  provides a yet stronger result.     In practice, the statistic requires a relatively large number of data points  to properly reject the null hypothesis.       The Kolmogorov distribution is the distribution of the  |random_variable|Random_Variable|      : K      where B is the  |Brownian_bridge|Brownian_Bridge| . The  |cumulative_distribution_function|Cumulative_Distribution_Function|  of K is given by      :   e  e,     which can also be expressed by the  |Jacobi_theta_function|Jacobi_Theta_Function|    . Both the form of the Kolmogorov–Smirnov test statistic and its asymptotic distribution under the null hypothesis were published by  |Andrey_Kolmogorov|Andrey_Kolmogorov| ,  while a table of the distribution was published by  |Nikolai_Smirnov|Nikolai_Smirnov| .  Recurrence relations for the distribution of the test statistic in finite samples are available.     Under null hypothesis that the sample comes from the hypothesized distribution F,     :    B       |in_distribution|Convergence_Of_Random_Variables| , where B is the  |Brownian_bridge|Brownian_Bridge| .     If F is continuous then under the null hypothesis   converges to the Kolmogorov distribution, which does not depend on F. This result may also be known as the Kolmogorov theorem. The accuracy of this limit as an approximation to the exact cdf of K when n is finite is not very impressive: even when n1000 , the corresponding maximum error is about 0.9  ; this error increases to 2.6  when n100 and to a totally unacceptable 7  when n10 . However, a very simple expedient of replacing x by     : x+        in the argument of the Jacobi theta function reduces these errors to   0.003  , 0.027  , and 0.27  respectively; such accuracy would be usually considered more than adequate for all practical applications.      The goodness-of-fit test or the Kolmogorov–Smirnov test can be constructed by using the critical values of the Kolmogorov distribution. This test is asymptotically valid when n   . It rejects the null hypothesis at level   if     :   /math  sub  /sub math /math  math /math math /math math /math ref nameSL2011  /ref ref  /ref ref nameSL2011/ ref nameDKT2019  /ref ref nameKSgeneral  /ref  ref nameDKT2019/  sub  /sub  ref name Pearson & Hartley  /ref  ref name Shorak & Wellner  /ref  sub /sub sub /sub sub /sub math /math math /math math /math math /math math /math math /math math /math math /math math /math ref nameDKT2019/ ref nameKSgeneral/  code /code code /code code /code ref namearnold-emerson  /ref  code /code ref  /ref  code /code ref  /ref math /math  math /math math /math math /math  math /math  math /math math  /math math /math  sup  /sup ref name Peacock  /ref  /ref   ref .