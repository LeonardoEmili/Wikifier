[
    {
        "In ": null
    },
    {
        "numerical analysis": "numerical analysis"
    },
    {
        ", Newtons method, also known as the Newton–Raphson method, named after ": null
    },
    {
        "Isaac Newton": "isaac newton"
    },
    {
        " and ": null
    },
    {
        "Joseph Raphson": "joseph raphson"
    },
    {
        ", is a ": null
    },
    {
        "root-finding algorithm": "root-finding algorithm"
    },
    {
        " which produces successively better ": null
    },
    {
        "approximations": "numerical analysis"
    },
    {
        " to the ": null
    },
    {
        "roots": "root of a function"
    },
    {
        " of a ": null
    },
    {
        "real": "real number"
    },
    {
        "-valued ": null
    },
    {
        "function": "function"
    },
    {
        ". The most basic version starts with a single-variable function  defined for a ": null
    },
    {
        "real variable": "real number"
    },
    {
        " , the functions ": null
    },
    {
        "derivative": "derivative"
    },
    {
        " , and an initial guess  for a ": null
    },
    {
        "root": "zero_of_a_function"
    },
    {
        " of . If the function satisfies sufficient assumptions and the initial guess is close, then     : x_ x_0 - \\frac \\,.     is a better approximation of the root than . Geometrically,  is the intersection of the -axis and the ": null
    },
    {
        "tangent": "tangent"
    },
    {
        " of the ": null
    },
    {
        "graph": "graph of a function"
    },
    {
        " of  at : that is, the improved guess is the unique root of the ": null
    },
    {
        "linear approximation": "linear approximation"
    },
    {
        " at the initial point. The process is repeated as     : x_ x_n - \\frac \\,     until a sufficiently precise value is reached. This algorithm is first in the class of ": null
    },
    {
        "Householders method": "householders method"
    },
    {
        "s, succeeded by ": null
    },
    {
        "Halleys method": "halleys method"
    },
    {
        ". The method can also be extended to ": null
    },
    {
        "complex functions": "complex-valued function"
    },
    {
        " and to ": null
    },
    {
        "systems of equations": "systems of equations"
    },
    {
        ".               The idea is to start with an initial guess which is reasonably close to the true root, then to approximate the function by its ": null
    },
    {
        "tangent line": "tangent line"
    },
    {
        " using ": null
    },
    {
        "calculus": "calculus"
    },
    {
        ", and finally to compute the -intercept of this tangent line by ": null
    },
    {
        "elementary algebra": "elementary algebra"
    },
    {
        ". This -intercept will typically be a better approximation to the original functions root than the first guess, and the method can be ": null
    },
    {
        "iterated": "iterative method"
    },
    {
        ".     More formally, suppose  is a ": null
    },
    {
        "differentiable": "derivative"
    },
    {
        " function defined on the ": null
    },
    {
        "interval": "interval"
    },
    {
        "  with values in the ": null
    },
    {
        "real numbers": "real number"
    },
    {
        " , and we have some current approximation . Then we can derive the formula for a better approximation,  by referring to the diagram on the right. The equation of the ": null
    },
    {
        "tangent line": "tangent line"
    },
    {
        " to the curve  at  is     : y f \\, + f,     where  denotes the ": null
    },
    {
        "derivative": "derivative"
    },
    {
        ". The -intercept of this line  is taken as the next approximation, , to the root, so that the equation of the tangent line is satisfied when :     : 0 f \\, + f.     Solving for  gives   : x_ x_n - \\frac.     We start the process with some arbitrary initial value .  The method will usually converge, provided this initial guess is close enough to the unknown zero, and that . Furthermore, for a zero of ": null
    },
    {
        "multiplicity": "multiplicity"
    },
    {
        "  1, the convergence is at least quadratic  in a ": null
    },
    {
        "neighbourhood": "neighbourhood"
    },
    {
        " of the zero, which intuitively means that the number of correct digits roughly doubles in every step. More details can be found in the ": null
    },
    {
        "analysis section": "analysis"
    },
    {
        " below.     ": null
    },
    {
        "Householders methods": "householders method"
    },
    {
        "are similar but have higher order for even faster convergence. However, the extra computations required for each step can slow down the overall performance relative to Newtons method, particularly if  or its derivatives are computationally expensive to evaluate.       The name Newtons method is derived from ": null
    },
    {
        "Isaac Newtons": "isaac newton"
    },
    {
        "description of a special case of the method in  De analysi per aequationes numero terminorum infinitas   and in De metodis fluxionum et serierum infinitarum . However, his method differs substantially from the modern method given above: Newton applies the method only to polynomials. He does not compute the successive approximations , but computes a sequence of polynomials, and only at the end arrives at an approximation for the root . Finally, Newton views the method as purely algebraic and makes no mention of the connection with calculus. Newton may have derived his method from a similar but less precise method by ": null
    },
    {
        "Vieta": "franciscus vieta"
    },
    {
        ". The essence of Vietas method can be found in the work of the ": null
    },
    {
        "Persian mathematician": "mathematics in medieval islam"
    },
    {
        " Sharaf al-Din al-Tusi": "sharaf al-din al-tusi"
    },
    {
        ", while his successor ": null
    },
    {
        "Jamshīd al-Kāshī": "jamshīd al-kāshī"
    },
    {
        " used a form of Newtons method to solve  to find roots of  . A special case of Newtons method for calculating square roots was known since ancient times and is often called the ": null
    },
    {
        "Babylonian method": "babylonian method"
    },
    {
        ".     Newtons method was used by 17th-century Japanese mathematician ": null
    },
    {
        "Seki Kōwa": "seki kōwa"
    },
    {
        " to solve single-variable equations, though the connection with calculus was missing.       Newtons method was first published in 1685 in A Treatise of Algebra both Historical and Practical by ": null
    },
    {
        "John Wallis": "john wallis"
    },
    {
        ".  In 1690, ": null
    },
    {
        "Joseph Raphson": "joseph raphson"
    },
    {
        " published a simplified description in Analysis aequationum universalis.  Raphson again viewed Newtons method purely as an algebraic method and restricted its use to polynomials, but he describes the method in terms of the successive approximations  instead of the more complicated sequence of polynomials used by Newton. Finally, in 1740, ": null
    },
    {
        "Thomas Simpson": "thomas simpson"
    },
    {
        " described Newtons method as an iterative method for solving general nonlinear equations using calculus, essentially giving the description above. In the same publication, Simpson also gives the generalization to systems of two equations and notes that Newtons method can be used for solving optimization problems by setting the gradient to zero.     ": null
    },
    {
        "Arthur Cayley": "arthur cayley"
    },
    {
        " in 1879 in The Newton–Fourier imaginary problem was the first to notice the difficulties in generalizing Newtons method to complex roots of polynomials with degree greater than 2 and complex initial values. This opened the way to the study of the ": null
    },
    {
        "theory of iterations": "julia set"
    },
    {
        " of rational functions.       Newtons method is an extremely powerful technique—in general the ": null
    },
    {
        "convergence": "rate of convergence"
    },
    {
        " is quadratic: as the method converges on the root, the difference between the root and the approximation is squared at each step. However, there are some difficulties with the method.       Newtons method requires that the derivative can be calculated directly. An analytical expression for the derivative may not be easily obtainable or could be expensive to evaluate. In these situations, it may be appropriate to approximate the derivative by using the slope of a line through two nearby points on the function. Using this approximation would result in something like the ": null
    },
    {
        "secant method": "secant method"
    },
    {
        " whose convergence is slower than that of Newtons method.       It is important to review the ": null
    },
    {
        "proof of quadratic convergence": "proof of quadratic convergence for newtons iterative method"
    },
    {
        " of Newtons Method before implementing it. Specifically, one should review the assumptions made in the proof. For ": null
    },
    {
        "situations where the method fails to converge": "failure analysis"
    },
    {
        ", it is because the assumptions made in this proof are not met.       If the first derivative is not well behaved in the neighborhood of a particular root, the method may overshoot, and diverge from that root. An example of a function with one root, for which the derivative is not well behaved in the neighborhood of the root, is     : f|x|^a,\\quad 0 ": null
    }
]