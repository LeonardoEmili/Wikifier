{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import bz2\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" Returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" Instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" Update mapping dicts based on the token \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\" Add a list of tokens into the Vocabulary \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" Retrieve the index associated with the token \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" Return the token associated with the index \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\", mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\", end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "\n",
    "    test_slice = 0.15\n",
    "    val_slice = 0.15\n",
    "    inner_sep = '_'\n",
    "    outer_sep = '|'\n",
    "    link_cutoff = 1\n",
    "    default_no_link = \"<NONE>\"\n",
    "\n",
    "    def __init__(self, dataset, vectorizer, strip_punctuation):\n",
    "        \n",
    "        self._dataset = dataset\n",
    "        self._vectorizer = vectorizer\n",
    "        self._strip_punctuation = strip_punctuation\n",
    "\n",
    "        self.train_ds = self._dataset['train']\n",
    "        self.train_size = len(self.train_ds['source_sentences'])\n",
    "\n",
    "        self.val_ds = self._dataset['val']\n",
    "        self.val_size = len(self.val_ds['source_sentences'])\n",
    "\n",
    "\n",
    "        self.test_ds = self._dataset['test']\n",
    "        self.test_size = len(self.test_ds['source_sentences'])\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_ds, self.train_size),\n",
    "                             'val': (self.val_ds, self.val_size),\n",
    "                             'test': (self.test_ds, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def is_a_link(cls, word):\n",
    "        return len(word) >= 2 and word[0] == cls.outer_sep and word[-1] == cls.outer_sep\n",
    "\n",
    "    @classmethod\n",
    "    def pre_process(cls, text):\n",
    "\n",
    "        # Text tokenizing\n",
    "        source_sentences = []\n",
    "        valid_links = []\n",
    "        for source_sentence in nltk.sent_tokenize(text):\n",
    "            tokens = nltk.word_tokenize(source_sentence)\n",
    "            for i in range(len(tokens)):\n",
    "                tokens[i] = tokens[i].lower()\n",
    "                if cls.is_a_link(tokens[i]):\n",
    "                    valid_links.append(tokens[i].split(cls.outer_sep)[-2])\n",
    "            source_sentences.append(tokens)\n",
    "\n",
    "        # Valid links is a set of the links which occur more than the treshold\n",
    "        valid_links = Counter(valid_links)\n",
    "        valid_links = set(link for link,frequence in valid_links.items() if frequence >= cls.link_cutoff)\n",
    "        sentences = []\n",
    "        labels = []\n",
    "\n",
    "        # Form input and label sequences\n",
    "        for i, source_sentence in enumerate(source_sentences):\n",
    "            sentence = []\n",
    "            label = []\n",
    "            for j, word in enumerate(source_sentence):\n",
    "                if cls.is_a_link(word):\n",
    "                    _split = list(filter(None, word.split(cls.outer_sep)))\n",
    "                    if len(_split) == 2:\n",
    "                        text, link = _split\n",
    "                        sub_links = filter(None, text.split(cls.inner_sep))\n",
    "                        link = link.replace(\"_\", \" \") if link in valid_links else cls.default_no_link\n",
    "                        for sub_link in sub_links:\n",
    "                            label.append(link)\n",
    "                            sentence.append(sub_link)\n",
    "                    else:\n",
    "                        word = word.replace(cls.outer_sep, '')\n",
    "                        label.append(cls.default_no_link)\n",
    "                        sentence.append(word)\n",
    "                else:\n",
    "                    label.append(cls.default_no_link)\n",
    "                    sentence.append(word)\n",
    "            labels.append(label)\n",
    "            sentences.append(sentence)\n",
    "        return sentences, labels\n",
    "\n",
    "    @classmethod\n",
    "    def read_dataset(cls, ds_path):\n",
    "        \n",
    "        text = bz2.BZ2File(ds_path).read().decode('utf-8')\n",
    "        sentences, labels = cls.pre_process(text)\n",
    "        train_size = int(len(sentences) * (1 - cls.test_slice - cls.val_slice))\n",
    "        test_size = int(len(sentences) * cls.test_slice)\n",
    "        return {\n",
    "            'train': {'source_sentences': sentences[:train_size], 'target_labels' : labels[:train_size]},\n",
    "            'test': {'source_sentences': sentences[train_size:train_size+test_size], 'target_labels' : labels[train_size:train_size+test_size]},\n",
    "            'val': {'source_sentences' : sentences[train_size+test_size:], 'target_labels' : labels[train_size+test_size:]}\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, ds_path, strip_punctuation=True):\n",
    "        \"\"\" Load dataset and make a new vectorizer from scratch \"\"\"\n",
    "\n",
    "        ds = cls.read_dataset(ds_path)\n",
    "        return cls(ds, Vectorizer.from_dataframe(ds), strip_punctuation)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" Selects the splits in the dataset\n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_ds, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "\n",
    "        sentence = self._target_ds['source_sentences'][index]\n",
    "        links = self._target_ds['target_labels'][index]\n",
    "        vector_dict = self._vectorizer.vectorize(sentence, links)\n",
    "\n",
    "        return {'x_source': vector_dict['source_vector'], 'y_target': vector_dict['target_vector'], 'x_source_length' : vector_dict['source_length']}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\"\"\"\n",
    "        return len(self) // batch_size  \n",
    "        \n",
    "    def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        A generator function which wraps the PyTorch DataLoader. It will \n",
    "        ensure each tensor is on the write device location.\n",
    "        \"\"\"\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "        for data_dict in dataloader:\n",
    "            lengths = data_dict['x_source_length'].numpy()\n",
    "            sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "            \n",
    "            out_data_dict = dict()\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"        \n",
    "    def __init__(self, source_vocab, target_vocab, max_source_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_vocab (SequenceVocabulary): maps source words to integers\n",
    "            target_vocab (SequenceVocabulary): maps target words to integers\n",
    "            max_source_length (int): the longest sequence in the source dataset\n",
    "        \"\"\"\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.max_source_length = max_source_length\n",
    "        \n",
    "\n",
    "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
    "        \"\"\"Vectorize the provided indices\n",
    "        \n",
    "        Args:\n",
    "            indices (list): a list of integers that represent a sequence\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "            mask_index (int): the mask_index to use; almost always 0\n",
    "        \"\"\"\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        vector[len(indices):] = mask_index\n",
    "        return vector\n",
    "\n",
    "    def _get_indices(self, tokens):\n",
    "        indices = [self.source_vocab.begin_seq_index]\n",
    "        indices.extend(self.source_vocab.lookup_token(token) for token in tokens)\n",
    "        indices.append(self.source_vocab.end_seq_index)\n",
    "        return indices\n",
    "\n",
    "        \n",
    "    def vectorize(self, source_words, target_links, vector_length=-1):\n",
    "        \"\"\"Return the vectorized source and target vectors\n",
    "        Args:\n",
    "            source_words (list): text tokens from the source vocabulary\n",
    "            target_links (list): link tokens from the target vocabulary\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            A tuple: (source_vector, target_vector)\n",
    "        \"\"\"\n",
    "\n",
    "        source_indices = self._get_indices(source_words)\n",
    "        target_indices = self._get_indices(target_links)\n",
    "\n",
    "        source_vector = self._vectorize(source_indices, vector_length=vector_length, mask_index=self.source_vocab.mask_index)\n",
    "        target_vector = self._vectorize(target_indices, vector_length=vector_length, mask_index=self.target_vocab.mask_index)\n",
    "\n",
    "        return {'source_vector': source_vector, \n",
    "                'target_vector': target_vector, \n",
    "                'source_length': len(source_indices)}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, ds):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            bitext_df (pandas.DataFrame): the parallel text dataset\n",
    "        Returns:\n",
    "            an instance of the NMTVectorizer\n",
    "        \"\"\"\n",
    "        source_vocab = SequenceVocabulary()\n",
    "        target_vocab = SequenceVocabulary()\n",
    "        max_source_length = 0\n",
    "\n",
    "        for _, split in ds.items():\n",
    "            for source_sequence in split['source_sentences']:\n",
    "                max_source_length = max(max_source_length, len(source_sequence))\n",
    "                for token in source_sequence:\n",
    "                    source_vocab.add_token(token)\n",
    "\n",
    "            for target_sequence in split['target_labels']:\n",
    "                for token in target_sequence:\n",
    "                    target_vocab.add_token(token)\n",
    "            \n",
    "        return cls(source_vocab, target_vocab, max_source_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        source_vocab = SequenceVocabulary.from_serializable(contents[\"source_vocab\"])\n",
    "        target_vocab = SequenceVocabulary.from_serializable(contents[\"target_vocab\"])\n",
    "        return cls(source_vocab=source_vocab, \n",
    "                   target_vocab=target_vocab, \n",
    "                   max_source_length=contents[\"max_source_length\"])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"source_vocab\": self.source_vocab.to_serializable(), \n",
    "                \"target_vocab\": self.target_vocab.to_serializable(), \n",
    "                \"max_source_length\": self.max_source_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset.load_dataset_and_make_vectorizer(\"../input_data/wiki.txt.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'source_vector': array([2, 1, 3]),\n 'target_vector': array([     2, 111494,      3]),\n 'source_length': 3}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y = 672\n",
    "#list(zip(dataset.train_ds[\"source_sentences\"][y], dataset.train_ds[\"target_labels\"][y]))\n",
    "\n",
    "dataset.get_vectorizer().vectorize([\"yasa\"], [\"yyy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}