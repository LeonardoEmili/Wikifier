{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import bz2\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processa(text, outer_sep, inner_sep, link_cutoff, default_no_link):\n",
    "\n",
    "    # Text tokenizing\n",
    "    source_sentences = []\n",
    "    valid_links = []\n",
    "    for source_sentence in nltk.sent_tokenize(text):\n",
    "        tokens = nltk.word_tokenize(source_sentence)\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "            if is_a_link(tokens[i], outer_sep):\n",
    "                valid_links.append(tokens[i].split(outer_sep)[-2])\n",
    "        source_sentences.append(tokens)\n",
    "\n",
    "    # Valid links is a set of the links which occur more than the treshold\n",
    "    valid_links = Counter(valid_links)\n",
    "    valid_links = set(link for link,frequence in valid_links.items() if frequence >= link_cutoff)\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Form input and label sequences\n",
    "    for i, source_sentence in enumerate(source_sentences):\n",
    "        sentence = []\n",
    "        label = []\n",
    "        for j, word in enumerate(source_sentence):\n",
    "            if is_a_link(word, outer_sep):\n",
    "                _split = list(filter(None, word.split(outer_sep)))\n",
    "                if len(_split) == 2:\n",
    "                    text, link = _split\n",
    "                    sub_links = filter(None, text.split(inner_sep))\n",
    "                    link = link.replace(\"_\", \" \") if link in valid_links else default_no_link\n",
    "                    for sub_link in sub_links:\n",
    "                        label.append(link)\n",
    "                        sentence.append(sub_link)\n",
    "                else:\n",
    "                    word = word.replace(outer_sep, '')\n",
    "                    label.append(default_no_link)\n",
    "                    sentence.append(word)\n",
    "            else:\n",
    "                label.append(default_no_link)\n",
    "                sentence.append(word)\n",
    "        labels.append(label)\n",
    "        sentences.append(sentence)\n",
    "    return sentences, labels\n",
    "\n",
    "def is_a_link(word, outer_sep):\n",
    "        return len(word) >= 2 and word[0] == outer_sep and word[-1] == outer_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "\n",
    "    test_slice = 0.15\n",
    "    val_slice = 0.15\n",
    "    inner_sep = '_'\n",
    "    outer_sep = '|'\n",
    "    link_cutoff = 1\n",
    "    default_no_link = \"<NONE>\"\n",
    "\n",
    "    def __init__(self, dataset):#, vectorizer):\n",
    "        \n",
    "        self._dataset = dataset\n",
    "        #self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_ds = self._dataset['train']\n",
    "        self.train_size = len(self.train_ds['source_sentences'])\n",
    "\n",
    "        self.val_ds = self._dataset['val']\n",
    "        self.val_size = len(self.val_ds['source_sentences'])\n",
    "\n",
    "\n",
    "        self.test_ds = self._dataset['test']\n",
    "        self.test_size = len(self.test_ds['source_sentences'])\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_ds, self.train_size),\n",
    "                             'val': (self.val_ds, self.val_size),\n",
    "                             'test': (self.test_ds, self.test_size)}\n",
    "\n",
    "        #self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def is_a_link(cls, word):\n",
    "        return len(word) >= 2 and word[0] == cls.outer_sep and word[-1] == cls.outer_sep\n",
    "\n",
    "    @classmethod\n",
    "    def read_dataset(cls, ds_path):\n",
    "        \n",
    "        text = bz2.BZ2File(ds_path).read().decode('utf-8')\n",
    "        sentences, labels = cls.pre_process(text)\n",
    "        train_size = int(len(sentences) * (1 - cls.test_slice - cls.val_slice))\n",
    "        test_size = int(len(sentences) * cls.test_slice)\n",
    "        return {\n",
    "            'train': {'source_sentences': sentences[:train_size], 'target_labels' : labels[:train_size]},\n",
    "            'test': {'source_sentences': sentences[train_size:train_size+test_size], 'target_labels' : labels[train_size:train_size+test_size]},\n",
    "            'val': {'source_sentences' : sentences[train_size+test_size:], 'target_labels' : labels[train_size+test_size:]}\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def pre_process(cls, text):\n",
    "\n",
    "        # Text tokenizing\n",
    "        source_sentences = []\n",
    "        valid_links = []\n",
    "        for source_sentence in nltk.sent_tokenize(text):\n",
    "            tokens = nltk.word_tokenize(source_sentence)\n",
    "            for i in range(len(tokens)):\n",
    "                tokens[i] = tokens[i].lower()\n",
    "                if cls.is_a_link(tokens[i]):\n",
    "                    valid_links.append(tokens[i].split(cls.outer_sep)[-2])\n",
    "            source_sentences.append(tokens)\n",
    "\n",
    "        # Valid links is a set of the links which occur more than the treshold\n",
    "        valid_links = Counter(valid_links)\n",
    "        valid_links = set(link for link,frequence in valid_links.items() if frequence >= cls.link_cutoff)\n",
    "        sentences = []\n",
    "        labels = []\n",
    "\n",
    "        # Form input and label sequences\n",
    "        for i, source_sentence in enumerate(source_sentences):\n",
    "            sentence = []\n",
    "            label = []\n",
    "            for j, word in enumerate(source_sentence):\n",
    "                if cls.is_a_link(word):\n",
    "                    _split = list(filter(None, word.split(cls.outer_sep)))\n",
    "                    if len(_split) == 2:\n",
    "                        text, link = _split\n",
    "                        sub_links = filter(None, text.split(cls.inner_sep))\n",
    "                        link = link.replace(\"_\", \" \") if link in valid_links else cls.default_no_link\n",
    "                        for sub_link in sub_links:\n",
    "                            label.append(link)\n",
    "                            sentence.append(sub_link)\n",
    "                    else:\n",
    "                        word = word.replace(cls.outer_sep, '')\n",
    "                        label.append(cls.default_no_link)\n",
    "                        sentence.append(word)\n",
    "                else:\n",
    "                    label.append(cls.default_no_link)\n",
    "                    sentence.append(word)\n",
    "            labels.append(label)\n",
    "            sentences.append(sentence)\n",
    "        return sentences, labels\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, ds_path, strip_punctuation=True):\n",
    "        \"\"\" Load dataset and make a new vectorizer from scratch \"\"\"\n",
    "\n",
    "        ds = cls.read_dataset(ds_path)\n",
    "        return cls(ds)\n",
    "        #return cls(ds, ReviewVectorizer.from_dataframe(ds))\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  \n",
    "        \n",
    "    def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                        drop_last=True, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        A generator function which wraps the PyTorch DataLoader. It will \n",
    "        ensure each tensor is on the write device location.\n",
    "        \"\"\"\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset.load_dataset_and_make_vectorizer(\"../input_data/wiki.txt.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('they', '<NONE>'),\n ('are', '<NONE>'),\n ('generally', '<NONE>'),\n ('believed', '<NONE>'),\n ('to', '<NONE>'),\n ('have', '<NONE>'),\n ('been', '<NONE>'),\n ('a', '<NONE>'),\n ('germanic', 'germanic peoples'),\n ('tribe', 'germanic peoples'),\n ('originating', '<NONE>'),\n ('in', '<NONE>'),\n ('jutland', 'jutland'),\n (',', '<NONE>'),\n ('but', '<NONE>'),\n ('celtic', 'celts'),\n ('influences', '<NONE>'),\n ('have', '<NONE>'),\n ('also', '<NONE>'),\n ('been', '<NONE>'),\n ('suggested', '<NONE>'),\n ('.', '<NONE>')]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 671\n",
    "list(zip(dataset.train_ds[\"source_sentences\"][y], dataset.train_ds[\"target_labels\"][y]))"
   ]
  }
 ]
}