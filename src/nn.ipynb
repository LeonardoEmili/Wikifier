{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import bz2\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token})\n",
    "        return contents\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" Instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" Update mapping dicts based on the token \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\" Add a list of tokens into the Vocabulary \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" Return the token associated with the index \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "\n",
    "    test_slice = 0.15\n",
    "    val_slice = 0.15\n",
    "    inner_sep = '_'\n",
    "    outer_sep = '|'\n",
    "    link_cutoff = 3\n",
    "    default_no_link = \"<NONE>\"\n",
    "\n",
    "    def __init__(self, dataset, vectorizer, strip_punctuation):\n",
    "        \n",
    "        self._dataset = dataset\n",
    "        self._vectorizer = vectorizer\n",
    "        self._strip_punctuation = strip_punctuation\n",
    "\n",
    "        self.train_ds = self._dataset['train']\n",
    "        self.train_size = len(self.train_ds['source_sentences'])\n",
    "\n",
    "        self.val_ds = self._dataset['val']\n",
    "        self.val_size = len(self.val_ds['source_sentences'])\n",
    "\n",
    "        self.test_ds = self._dataset['test']\n",
    "        self.test_size = len(self.test_ds['source_sentences'])\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_ds, self.train_size),\n",
    "                             'val': (self.val_ds, self.val_size),\n",
    "                             'test': (self.test_ds, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def is_a_link(cls, word):\n",
    "        return len(word) >= 2 and word[0] == cls.outer_sep and word[-1] == cls.outer_sep\n",
    "\n",
    "    @classmethod\n",
    "    def pre_process(cls, text):\n",
    "\n",
    "        # Text tokenizing\n",
    "        source_sentences = []\n",
    "        valid_links = []\n",
    "        for source_sentence in nltk.sent_tokenize(text):\n",
    "            tokens = nltk.word_tokenize(source_sentence)\n",
    "            for i in range(len(tokens)):\n",
    "                tokens[i] = tokens[i].lower()\n",
    "                if cls.is_a_link(tokens[i]):\n",
    "                    valid_links.append(tokens[i].split(cls.outer_sep)[-2])\n",
    "            source_sentences.append(tokens)\n",
    "\n",
    "        # Valid links is a set of the links which occur more than the treshold\n",
    "        valid_links = Counter(valid_links)\n",
    "        valid_links = set(link for link,frequence in valid_links.items() if frequence >= cls.link_cutoff)\n",
    "        sentences = []\n",
    "        labels = []\n",
    "\n",
    "        # Form input and label sequences\n",
    "        for i, source_sentence in enumerate(source_sentences):\n",
    "            sentence = []\n",
    "            label = []\n",
    "            for j, word in enumerate(source_sentence):\n",
    "                if cls.is_a_link(word):\n",
    "                    _split = list(filter(None, word.split(cls.outer_sep)))\n",
    "                    if len(_split) == 2:\n",
    "                        text, link = _split\n",
    "                        sub_links = filter(None, text.split(cls.inner_sep))\n",
    "                        link = link.replace(\"_\", \" \") if link in valid_links else cls.default_no_link\n",
    "                        for sub_link in sub_links:\n",
    "                            label.append(link)\n",
    "                            sentence.append(sub_link)\n",
    "                    else:\n",
    "                        #word = word.replace(cls.outer_sep, '') # TODO check if inner sep still occur\n",
    "                        word = word.replace(cls.outer_sep, '').replace(cls.inner_sep, ' ')\n",
    "                        label.append(cls.default_no_link)\n",
    "                        sentence.append(word)\n",
    "                else:\n",
    "                    label.append(cls.default_no_link)\n",
    "                    sentence.append(word)\n",
    "            labels.append(label)\n",
    "            sentences.append(sentence)\n",
    "        return sentences, labels\n",
    "\n",
    "    @classmethod\n",
    "    def read_dataset(cls, ds_path):\n",
    "        \n",
    "        text = bz2.BZ2File(ds_path).read().decode('utf-8')\n",
    "        sentences, labels = cls.pre_process(text)\n",
    "        train_size = int(len(sentences) * (1 - cls.test_slice - cls.val_slice))\n",
    "        test_size = int(len(sentences) * cls.test_slice)\n",
    "        return {\n",
    "            'train': {'source_sentences': sentences[:train_size], 'target_labels' : labels[:train_size]},\n",
    "            'test': {'source_sentences': sentences[train_size:train_size+test_size], 'target_labels' : labels[train_size:train_size+test_size]},\n",
    "            'val': {'source_sentences' : sentences[train_size+test_size:], 'target_labels' : labels[train_size+test_size:]}\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, ds_path, strip_punctuation=True):\n",
    "        \"\"\" Load dataset and make a new vectorizer from scratch \"\"\"\n",
    "\n",
    "        ds = cls.read_dataset(ds_path)\n",
    "        return cls(ds, Vectorizer.from_dataframe(ds), strip_punctuation)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" Selects the splits in the dataset\n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_ds, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "\n",
    "        sentence = self._target_ds['source_sentences'][index]\n",
    "        links = self._target_ds['target_labels'][index]\n",
    "        vector_dict = self._vectorizer.vectorize(sentence, links)\n",
    "        return {'x_source': vector_dict['source_vector'], 'y_target': vector_dict['target_vector'], 'x_source_length' : vector_dict['source_length']}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\"\"\"\n",
    "        return len(self) // batch_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict['x_source_length'].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "        \n",
    "        out_data_dict = dict()\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"        \n",
    "    def __init__(self, source_vocab, target_vocab, max_source_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_vocab (SequenceVocabulary): maps source words to integers\n",
    "            target_vocab (SequenceVocabulary): maps target words to integers\n",
    "            max_source_length (int): the longest sequence in the source dataset\n",
    "        \"\"\"\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.max_source_length = max_source_length\n",
    "        \n",
    "\n",
    "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
    "        \"\"\"Vectorize the provided indices\n",
    "        \n",
    "        Args:\n",
    "            indices (list): a list of integers that represent a sequence\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "            mask_index (int): the mask_index to use; almost always 0\n",
    "        \"\"\"\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        vector[len(indices):] = mask_index\n",
    "        return vector\n",
    "\n",
    "    def _get_indices(self, tokens, vocab):\n",
    "        #indices = [vocab.begin_seq_index]\n",
    "        indices = []\n",
    "        indices.extend(vocab.lookup_token(token) for token in tokens)\n",
    "        #indices.append(vocab.end_seq_index)\n",
    "        return indices\n",
    "\n",
    "        \n",
    "    def vectorize(self, source_words, target_links, vector_length=-1):\n",
    "        \"\"\"Return the vectorized source and target vectors\n",
    "        Args:\n",
    "            source_words (list): text tokens from the source vocabulary\n",
    "            target_links (list): link tokens from the target vocabulary\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            A tuple: (source_vector, target_vector)\n",
    "        \"\"\"\n",
    "\n",
    "        source_indices = self._get_indices(source_words, self.source_vocab)\n",
    "        target_indices = self._get_indices(target_links, self.target_vocab)\n",
    "\n",
    "        vector_length = self.max_source_length\n",
    "\n",
    "        source_vector = self._vectorize(source_indices, vector_length=vector_length, mask_index=self.source_vocab.mask_index)\n",
    "        target_vector = self._vectorize(target_indices, vector_length=vector_length, mask_index=self.target_vocab.mask_index)\n",
    "\n",
    "        return {'source_vector': source_vector, \n",
    "                'target_vector': target_vector, \n",
    "                'source_length': len(source_indices)}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, ds):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            bitext_df (pandas.DataFrame): the parallel text dataset\n",
    "            TODO update ds description\n",
    "        Returns:\n",
    "            an instance of the NMTVectorizer\n",
    "        \"\"\"\n",
    "        source_vocab = Vocabulary()\n",
    "        target_vocab = Vocabulary()\n",
    "        max_source_length = 0\n",
    "\n",
    "        '''\n",
    "        TODO check: This version adds token for each of the splits\n",
    "        for _, split in ds.items():\n",
    "            for source_sequence in split['source_sentences']:\n",
    "                max_source_length = max(max_source_length, len(source_sequence))\n",
    "                for token in source_sequence:\n",
    "                    source_vocab.add_token(token)\n",
    "\n",
    "            for target_sequence in split['target_labels']:\n",
    "                for token in target_sequence:\n",
    "                    target_vocab.add_token(token)\n",
    "        '''\n",
    "\n",
    "        for _, split in ds.items():\n",
    "            for source_sequence in split['source_sentences']:\n",
    "                max_source_length = max(max_source_length, len(source_sequence))\n",
    "        \n",
    "        for source_sequence in ds['train']['source_sentences']:\n",
    "            for token in source_sequence:\n",
    "                source_vocab.add_token(token)\n",
    "\n",
    "        for target_sequence in ds['train']['target_labels']:\n",
    "            for token in target_sequence:\n",
    "                target_vocab.add_token(token)\n",
    "            \n",
    "        return cls(source_vocab, target_vocab, max_source_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        source_vocab = SequenceVocabulary.from_serializable(contents[\"source_vocab\"])\n",
    "        target_vocab = SequenceVocabulary.from_serializable(contents[\"target_vocab\"])\n",
    "        return cls(source_vocab=source_vocab, \n",
    "                   target_vocab=target_vocab, \n",
    "                   max_source_length=contents[\"max_source_length\"])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"source_vocab\": self.source_vocab.to_serializable(), \n",
    "                \"target_vocab\": self.target_vocab.to_serializable(), \n",
    "                \"max_source_length\": self.max_source_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, vocab_size, embedding_dim, padding_idx, hidden_dim, target_size, batch_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, self.target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # The weights are of the form (num_layers * num_directions, minibatch_size ,hidden_dim)\n",
    "        return (torch.randn(self.num_layers * 2, self.batch_size, self.hidden_dim), torch.randn(self.num_layers * 2, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sequences, lengths):\n",
    "        # Reset LSTM hidden state, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        # Dim transformation: (batch_size, seq_size, 1) -> (batch_size, seq_size, embedding_dim)\n",
    "        embeds = self.word_embedding(sequences)\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        # Please note that output_lengths are the original 'lengths'\n",
    "        lstm_out, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        batch_size, seq_size, feat_size = lstm_out.shape\n",
    "\n",
    "        # Dim transformation: (batch_size, seq_size, hidden_size * 2) -> (batch_size * seq_size, hidden_size * 2)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        link_outputs = self.fc(lstm_out)\n",
    "        link_scores = F.log_softmax(link_outputs, dim=1)\n",
    "        \n",
    "        # Dim transformation: (batch_size * seq_size, target_size) -> (batch_size, seq_size, target_size)\n",
    "        link_scores = link_scores.view(batch_size, seq_size, self.target_size)\n",
    "\n",
    "        if (seq_size < vectorizer.max_source_length):\n",
    "            extension = torch.autograd.Variable(torch.zeros(batch_size, vectorizer.max_source_length-seq_size, self.target_size))\n",
    "            x = torch.cat([x, extension], 1)\n",
    "        return link_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ce422d2abff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#print(sequences[0].shape, y[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "#sample = b['x_source'][:2]\n",
    "#length = b['x_source_length'][:2]\n",
    "#print(sample.shape)\n",
    "#print(sample, length)\n",
    "#print(sample.size())\n",
    "#print(\"---\")\n",
    "#model(sample, length)\n",
    "\n",
    "\n",
    "sequences = sample\n",
    "\n",
    "#print(sequences[0].shape, y[0].shape)\n",
    "\n",
    "lengths = length\n",
    "#batch_size, seq_len = sequences.size()\n",
    "x = model.word_embedding(sample)\n",
    "print(x.shape)\n",
    "x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "#hidden = model.init_hidden()\n",
    "x, hidden = model.lstm(x)\n",
    "x, s = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "print('Unpacked size: ',x.shape, s)\n",
    "batch_size, seq_len, feat_size = x.shape\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "x = x.contiguous().view(batch_size * seq_len, feat_size)\n",
    "#x = x.contiguous().view(-1, x.shape[2])\n",
    "print(x.shape)\n",
    "\n",
    "x = model.fc(x)\n",
    "print(x.shape)\n",
    "x = F.log_softmax(x, dim=1)\n",
    "\n",
    "print('After linear: ', x.shape)\n",
    "x = x.view(batch_size, seq_len, model.target_size)\n",
    "\n",
    "if (seq_len < vectorizer.max_source_length):\n",
    "    # lstm_out --> seqlen X bsz X hidden_dim\n",
    "    print('asd')\n",
    "    extension = torch.autograd.Variable(torch.zeros(batch_size, vectorizer.max_source_length-seq_len, model.target_size))\n",
    "    print(x.shape, extension.shape)\n",
    "    x = torch.cat([x, extension], 1)\n",
    "    #print(seq_len, vectorizer.max_source_length)\n",
    "    print('@@ ', x.shape)\n",
    "\n",
    "print(x.shape)\n",
    "#print(y[0,67])\n",
    "print(y.shape)\n",
    "#print(x.shape)\n",
    "\n",
    "\n",
    "#model(batch_dict['x_source'], batch_dict['x_source_length'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset.load_dataset_and_make_vectorizer(\"../input_data/wiki.txt.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_hat, y_true, mask_index):\n",
    "\n",
    "    _, y_hat = y_hat.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_hat, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = 672\n",
    "#list(zip(dataset.train_ds[\"source_sentences\"][y], dataset.train_ds[\"target_labels\"][y]))\n",
    "\n",
    "#dataset.get_vectorizer().vectorize([\"xd\"], [\"disney xd\"])\n",
    "#dataset.set_split('val')\n",
    "#print(dataset.get_vectorizer().vectorize([\"xd\"], [\"xd\"]))\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "device = 'cpu'\n",
    "learning_rate = 5e-4\n",
    "\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "model = BiLSTM(num_layers=1, vocab_size=len(vectorizer.source_vocab), embedding_dim=embedding_dim, padding_idx=0, hidden_dim=128, target_size=len(vectorizer.target_vocab), batch_size=batch_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=batch_size, device=device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the output\n",
    "        y_hat = model(batch_dict['x_source'], batch_dict['x_source_length'])\n",
    "\n",
    "        # Compute loss using cross entropy\n",
    "        loss = F.cross_entropy(y_hat, batch_dict['y_target'], ignore_index=0)\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Take gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_hat, batch_dict['y_target'], mask_index=0)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#num_layers, vocab_size, embedding_dim, padding_idx, hidden_dim, target_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat = model(batch_dict['x_source'], batch_dict['x_source_length'])\n",
    "batch_generator = generate_batches(dataset, batch_size=batch_size, device=device)\n",
    "b = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[dataset.get_vectorizer().target_vocab.lookup_index(x) for x in range(800, 900)]\n",
    "#print(len(dataset.get_vectorizer().target_vocab))\n",
    "#print(dataset.get_vectorizer().target_vocab.lookup_index(102))\n",
    "\n",
    "#vectorizer = dataset.get_vectorizer()\n",
    "#item = dataset.__getitem__(0)\n",
    "#print(item)\n",
    "#print([vectorizer.source_vocab.lookup_index(i) for i in item['x_source']])\n",
    "#print(len([vectorizer.target_vocab.lookup_index(i) for i in item['y_target']]))\n",
    "\n",
    "\n",
    "sample = b['x_source'][:2]\n",
    "length = b['x_source_length'][:2]\n",
    "y = b['y_target'][:2]\n",
    "#print(sample.shape)\n",
    "#print(sample, length)\n",
    "#print(sample.size())\n",
    "#print(\"---\")\n",
    "#model(sample, length)\n",
    "\n",
    "\n",
    "sequences = sample\n",
    "lengths = length\n",
    "batch_size, seq_len = sequences.size()\n",
    "x = model.word_embedding(sample)\n",
    "\n",
    "# MODEL\n",
    "\n",
    "batch_size, seq_len = sequences.size()\n",
    "# Dim transformation: (batch_size, seq_len, 1) -> (batch_size, seq_len, embedding_dim)\n",
    "embeds = model.word_embedding(sequences)\n",
    "print('--- ', embeds.size())\n",
    "embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "#embeds = embeds.view(len(sequence), 1, -1)\n",
    "lstm_out, model.hidden = model.lstm(embeds)\n",
    "lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "print('--- ', lstm_out.size())\n",
    "\n",
    "# Dim transformation: (batch_size, seq_len, hidden_size) -> (batch_size * seq_len, hidden_size)\n",
    "\n",
    "print(lstm_out.shape)\n",
    "lstm_out = lstm_out.contiguous()\n",
    "lstm_out = lstm_out.view(-1, lstm_out.shape[2])\n",
    "print(lstm_out.shape)\n",
    "\n",
    "#lstm_out = lstm_out.view(len(sequence), -1)\n",
    "link_space = model.fc(lstm_out)\n",
    "# Dim transformation: (batch_size * seq_len, nb_lstm_units) -> (batch_size, seq_len, nb_tags)\n",
    "#print('link space', link_space.shape)\n",
    "link_scores = F.log_softmax(link_space, dim=1)\n",
    "\n",
    "# I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "y_hat = link_scores.view(batch_size, -1, model.target_size)\n",
    "\n",
    "print(y_hat.shape)\n",
    "y = b['y_target'][:2]\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "#loss = F.cross_entropy(y_hat, y, ignore_index=0)\n",
    "\n",
    "# MODEL"
   ]
  }
 ]
}