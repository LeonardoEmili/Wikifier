{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "2.7.17-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "wikification_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfFwIafQSFNj",
        "colab_type": "text"
      },
      "source": [
        "# Wikification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrNLkPY9YU-d",
        "colab_type": "text"
      },
      "source": [
        "This notebook cointains the code for the neural network which I had to implement as part of my project during the _Honours Programme_ at Sapienza. This task belongs to the category of _Named-entity recognition_ (NER) and the model aims to, given a sentence in natural language (written in english) as input, to recognize parts of the text into Wikipedia entities.\n",
        "\n",
        "The whole dataset was retrieved from public [XML dumps](https://dumps.wikimedia.org/enwiki/) and then turned into useful information using a small script called __Wikifier__ which simply parses these files and extracts lines of text as well as wikilinks. The final dataset is stored as a collection of 8 files located in the directory __input_data/__. I had to split the dataset into smaller files because I was always running into memory errors but also because it provides modularity (basically you can choose the part of the dataset to use for the training).\n",
        "\n",
        "The model was trained on a Tesla P100-PCIE-16GB x1 and was able to achieve approximately 91% of accuracy running for 15 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxIqO3SXSLRE",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMAhu2EIlQ40",
        "colab_type": "code",
        "outputId": "235686e0-64a1-4669-e76f-45f4908af1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "from tqdm import trange\n",
        "from glob import glob\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from functools import partial\n",
        "from IPython.core.display import display, HTML\n",
        "from contextlib import contextmanager,redirect_stderr\n",
        "from statistics import mean\n",
        "import json, os, re, string, bz2, nltk, torch, wikipedia, sys, requests, random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.ticker as mtick\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Comment lines below if not executing from Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KreCjqczSTNY",
        "colab_type": "text"
      },
      "source": [
        "## Data Vectorization classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8JKQ_0XSXJt",
        "colab_type": "text"
      },
      "source": [
        "###Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idle0UE1lQ48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiDataset(Dataset):\n",
        "\n",
        "    test_slice = 0.10\n",
        "    val_slice = 0.20\n",
        "    inner_sep = '_'\n",
        "    outer_sep = '|'\n",
        "    link_cutoff = 40\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        \n",
        "        self._dataset = dataset\n",
        "        self.train_ds = self._dataset['train']\n",
        "        self.val_ds = self._dataset['val']\n",
        "        self.test_ds = self._dataset['test']\n",
        "        self._lookup_dict = {'train': self.train_ds, 'val': self.val_ds, 'test': self.test_ds}\n",
        "        self.set_split('train')\n",
        "\n",
        "    def train_size(self):\n",
        "        return len(self.train_ds['source_sentences'])\n",
        "\n",
        "    def val_size(self):\n",
        "        return len(self.val_ds['source_sentences'])\n",
        "\n",
        "    def test_size(self):\n",
        "        return len(self.test_ds['source_sentences'])\n",
        "\n",
        "    @classmethod\n",
        "    def is_a_link(cls, word):\n",
        "        return len(word) >= 2 and word[0] == cls.outer_sep and word[-1] == cls.outer_sep\n",
        "\n",
        "    @classmethod\n",
        "    def pre_process(cls, text, strip_punctuation):\n",
        "        #punctuation = \"{[]()},.;:!?~^'\\\"\"\n",
        "        \n",
        "        source_sentences = [[token.lower() for token in nltk.word_tokenize(source_sentence)] for source_sentence in text.readlines()]\n",
        "        valid_links = Counter(token.split(cls.outer_sep)[-2] for source_sentence in source_sentences for token in source_sentence if cls.is_a_link(token))\n",
        "        valid_links = set(link for link,frequence in Counter(valid_links).items() if frequence >= cls.link_cutoff)\n",
        "        sentences = []\n",
        "        labels = []\n",
        "\n",
        "        for source_sentence in source_sentences:\n",
        "            sentence = []\n",
        "            label = []\n",
        "            has_valid_links = False\n",
        "            for token in source_sentence:\n",
        "                if cls.is_a_link(token):\n",
        "                    _split = list(filter(None, token.split(cls.outer_sep)))\n",
        "                    if len(_split) == 2:\n",
        "                        text, link = _split\n",
        "                        if link in valid_links:\n",
        "                            link = link.replace(\"_\", \" \")\n",
        "                            has_valid_links = True\n",
        "                        else:\n",
        "                            link = None\n",
        "                        sub_links = filter(None, text.split(cls.inner_sep))\n",
        "                        for sub_link in sub_links:\n",
        "                            label.append(link)\n",
        "                            sentence.append(sub_link)\n",
        "                    else:\n",
        "                        token = token.replace(cls.outer_sep, ' ').replace(cls.inner_sep, ' ')\n",
        "                        label.append(None)\n",
        "                        sentence.append(token)\n",
        "                else:\n",
        "                    label.append(None)\n",
        "                    sentence.append(token)\n",
        "            if has_valid_links:\n",
        "                labels.append(label)\n",
        "                sentences.append(sentence)\n",
        "        return sentences, labels\n",
        "\n",
        "    @classmethod\n",
        "    def read_dataset(cls, ds_path, strip_punctuation):\n",
        "        text = bz2.open(ds_path, mode='rt', encoding='utf-8')\n",
        "        sentences, labels = cls.pre_process(text, strip_punctuation)\n",
        "        train_size = int(len(sentences) * (1 - cls.test_slice - cls.val_slice))\n",
        "        test_size = int(len(sentences) * cls.test_slice)\n",
        "        return {\n",
        "            'train': {'source_sentences': sentences[:train_size], 'target_labels' : labels[:train_size]},\n",
        "            'test': {'source_sentences': sentences[train_size:train_size+test_size], 'target_labels' : labels[train_size:train_size+test_size]},\n",
        "            'val': {'source_sentences' : sentences[train_size+test_size:], 'target_labels' : labels[train_size+test_size:]}\n",
        "        }\n",
        "    \n",
        "    @classmethod\n",
        "    def load_from_files(cls, ds_dir, max_files=-1, file_ext=\"bz2\", strip_punctuation=False):\n",
        "        ds_dir = ds_dir + \"/\" if not ds_dir.endswith(\"/\") else ds_dir\n",
        "        dumps = sorted(glob(ds_dir + \"*.\" + file_ext))\n",
        "        max_files = len(dumps) if max_files <= -1 else max_files\n",
        "        ds = {\n",
        "            'train': {'source_sentences': [], 'target_labels' : []},\n",
        "            'test': {'source_sentences': [], 'target_labels' : []},\n",
        "            'val': {'source_sentences': [], 'target_labels' : []}\n",
        "        }\n",
        "        for i in trange(min(max(1, max_files), len(dumps)), leave=True, desc=\"Loading wikidumps\"):\n",
        "            path = dumps[i]\n",
        "            dump = cls.read_dataset(path, strip_punctuation)\n",
        "            for split in ds.keys():\n",
        "                ds[split]['source_sentences'] += dump[split]['source_sentences']\n",
        "                ds[split]['target_labels'] += dump[split]['target_labels']\n",
        "        return cls(ds)\n",
        "        \n",
        "\n",
        "    def reduce(self, dev):\n",
        "        train_mean = sum([len(x) for x in self.train_ds['source_sentences']]) / self.train_size()\n",
        "        val_mean = sum([len(x) for x in self.val_ds['source_sentences']]) / self.val_size()\n",
        "        test_mean = sum([len(x) for x in self.test_ds['source_sentences']]) / self.test_size()\n",
        "\n",
        "        self.train_ds['source_sentences'] = [x for x in self.train_ds['source_sentences'] if len(x) <= train_mean*dev]\n",
        "        self.train_ds['target_labels'] = [y for y in self.train_ds['target_labels'] if len(y) <= train_mean*dev]\n",
        "\n",
        "        self.val_ds['source_sentences'] = [x for x in self.val_ds['source_sentences'] if len(x) <= val_mean*dev]\n",
        "        self.val_ds['target_labels'] = [y for y in self.val_ds['target_labels'] if len(y) <= val_mean*dev]\n",
        "\n",
        "        self.test_ds['source_sentences'] = [x for x in self.test_ds['source_sentences'] if len(x) <= test_mean*dev]\n",
        "        self.test_ds['target_labels'] = [y for y in self.test_ds['target_labels'] if len(y) <= test_mean*dev]\n",
        "\n",
        "    def encode_from(self, vocabulary):\n",
        "\n",
        "        for i, sentence in enumerate(self.train_ds['source_sentences']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.train_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.val_ds['source_sentences']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.val_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.test_ds['source_sentences']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.test_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.train_ds['target_labels']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.train_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.val_ds['target_labels']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.val_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.test_ds['target_labels']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.test_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
        "\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\" Selects the splits in the dataset, from 'train', 'val' or 'test' \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_ds = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._target_ds['source_sentences'])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self._target_ds['source_sentences'][index]\n",
        "        links = self._target_ds['target_labels'][index]\n",
        "        return {'x_source': sentence, 'y_target': links, 'x_source_length' : len(sentence)}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\"\"\"\n",
        "        return len(self) // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGwki6e2Sg3R",
        "colab_type": "text"
      },
      "source": [
        "###Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP6DCd2PlQ5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "\n",
        "    _padding_token = '<PAD>'\n",
        "    _unknown_token = '<UNK>'\n",
        "    \n",
        "    def __init__(self, word_to_index, link_to_index):\n",
        "        self._word_to_index = word_to_index\n",
        "        self._link_to_index = link_to_index\n",
        "        self._index_to_word = {i:w for i,w in enumerate(word_to_index)}\n",
        "        self._index_to_link = {i:l for i,l in enumerate(link_to_index)}\n",
        "\n",
        "    def source_size(self):\n",
        "        return len(self._word_to_index)\n",
        "\n",
        "    def target_size(self):\n",
        "        return len(self._link_to_index)\n",
        "    \n",
        "    def lookup_word(self, word):\n",
        "        return self._word_to_index.get(word, 1)\n",
        "\n",
        "    def lookup_link(self, link):\n",
        "        return self._link_to_index[link]\n",
        "\n",
        "    def lookup_word_index(self, index):\n",
        "        return self._index_to_word.get(index, self._unknown_token)\n",
        "\n",
        "    def lookup_link_index(self, index):\n",
        "        return self._index_to_link[index]\n",
        "\n",
        "    @classmethod\n",
        "    def of(cls, ds):\n",
        "        source_vocab = dict()\n",
        "        target_vocab = dict()\n",
        "        source_vocab = {cls._padding_token : 0, cls._unknown_token : 1}\n",
        "        target_vocab = {None : 0}\n",
        "\n",
        "        # Add words to the Vocabulary from the training set only\n",
        "        for source_sequence in ds.train_ds['source_sentences']:\n",
        "            for token in source_sequence:\n",
        "                if token not in source_vocab:\n",
        "                    source_vocab[token] = len(source_vocab)\n",
        "\n",
        "        # Add links to the Vocabulary from train, validation and test set. In fact, \n",
        "        # this should not influence predictionsbut provides reliability to the model.\n",
        "        for target_sequence in ds.train_ds['target_labels']:\n",
        "            for token in target_sequence:\n",
        "                if token not in target_vocab:\n",
        "                    target_vocab[token] = len(target_vocab)\n",
        "\n",
        "        for target_sequence in ds.val_ds['target_labels']:\n",
        "            for token in target_sequence:\n",
        "                if token not in target_vocab:\n",
        "                    target_vocab[token] = len(target_vocab)\n",
        "\n",
        "        for target_sequence in ds.test_ds['target_labels']:\n",
        "            for token in target_sequence:\n",
        "                if token not in target_vocab:\n",
        "                    target_vocab[token] = len(target_vocab)\n",
        "\n",
        "        return cls(source_vocab, target_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgjnHXpySvYw",
        "colab_type": "text"
      },
      "source": [
        "##The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgUMHf7HlQ5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, batch_size, num_layers=1, num_directions=2, padding_idx=0, device='cpu'):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.target_size = target_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_directions = num_directions\n",
        "        self.device = device\n",
        "\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * num_directions, self.target_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(self.num_layers * self.num_directions, self.batch_size, self.hidden_dim).to(self.device), \\\n",
        "        torch.randn(self.num_layers * self.num_directions, self.batch_size, self.hidden_dim).to(self.device))\n",
        "    \n",
        "    def forward(self, sequences, lengths):\n",
        "        # Reset LSTM hidden state, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "        # Dim transformation: (batch_size, seq_size, 1) -> (batch_size, seq_size, embedding_dim)\n",
        "        embeds = self.word_embedding(sequences)\n",
        "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        # Please note that output_lengths are the original 'lengths'\n",
        "        lstm_out, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        batch_size, seq_size, feat_size = lstm_out.shape\n",
        "\n",
        "        # Dim transformation: (batch_size, seq_size, hidden_size * directions) -> (batch_size * seq_size, hidden_size * directions)\n",
        "        lstm_out = lstm_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "        # Current shape: (batch_size * seq_size, target_size)\n",
        "        link_outputs = self.fc(lstm_out)\n",
        "        return link_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmfCxT5PTLkW",
        "colab_type": "text"
      },
      "source": [
        "##Training Routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV8ftJ7_TN9G",
        "colab_type": "text"
      },
      "source": [
        "###Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcEctpbdlQ5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _vectorize(indices, padding_index, vector_length):\n",
        "    vector = np.zeros(vector_length, dtype=np.int)\n",
        "    vector[:len(indices)] = indices\n",
        "    vector[len(indices):] = padding_index\n",
        "    return vector.tolist()\n",
        "\n",
        "def vectorize(input_sequence, padding_index=0, vector_length=-1):\n",
        "    if vector_length < 0:\n",
        "        vector_length = input_sequence['x_source_length']\n",
        "\n",
        "    source_sequence = _vectorize(input_sequence['x_source'], padding_index, vector_length)\n",
        "    target_sequence = _vectorize(input_sequence['y_target'], padding_index, vector_length)\n",
        "\n",
        "    return {'x_source' : source_sequence, 'y_target' : target_sequence, 'x_source_length' : input_sequence['x_source_length']}\n",
        "\n",
        "def collate_fn(batch, device='cpu'):\n",
        "    batch.sort(key=lambda sample: sample['x_source_length'], reverse=True)\n",
        "    local_max_length = batch[0]['x_source_length']\n",
        "    batch = [vectorize(sequence, vector_length=local_max_length) for sequence in batch]\n",
        "    output_batch = {'x_source' : [], 'y_target' : [], 'x_source_length' : []}\n",
        "    for sample in batch:\n",
        "        output_batch['x_source'].append(sample['x_source'])\n",
        "        output_batch['y_target'].append(sample['y_target'])\n",
        "        output_batch['x_source_length'].append(sample['x_source_length'])\n",
        "    return {'x_source' : torch.LongTensor(output_batch['x_source']).to(device), 'y_target' : torch.LongTensor(output_batch['y_target']).to(device), 'x_source_length' : torch.LongTensor(output_batch['x_source_length']).to(device)}\n",
        "\n",
        "def compute_accuracy(y_hat, y, mask_index=0):\n",
        "    y_hat = torch.argmax(y_hat, dim=1)\n",
        "    y = y.view(-1)\n",
        "    correct_indices = torch.eq(y_hat, y).float()\n",
        "    valid_indices = torch.ne(y, mask_index).float()\n",
        "    n_correct = (correct_indices * valid_indices).sum().item()\n",
        "    n_valid = max(1, valid_indices.sum().item())\n",
        "    return n_correct / n_valid * 100\n",
        "\n",
        "def annotate(input, stopwords, args):\n",
        "    ''' Use input sentences as the input of our model '''\n",
        "\n",
        "    # Vectorize input\n",
        "    sentences = [nltk.word_tokenize(sentence) for sentence in nltk.sent_tokenize(input)]\n",
        "    lengths = torch.LongTensor([[len(sentence)] for sentence in sentences])\n",
        "    input_vectors = [torch.LongTensor([[vocabulary.lookup_word(word.lower()) for word in sentence]]).to(args.device) for sentence in sentences]\n",
        "\n",
        "    # Consider the output of the Net as a distribution of probability and take the links with the highest scores\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "    predictions = [softmax(model(sentence, length)) for i, (sentence, length) in enumerate(zip(input_vectors, lengths))]\n",
        "    predictions = [[tensor.tolist() for tensor in torch.topk(predictions[i], args.top_k, dim=1)] for i in range(len(predictions))]\n",
        "    scores, predictions = list(zip(*predictions))\n",
        "    predictions = [[[vocabulary.lookup_link_index(token_index) for token_index in token_indices] for token_indices in sequence_indices] for sequence_indices in predictions]\n",
        "\n",
        "    # Pick up links and form output sentences which contains hyperlinks\n",
        "    output_sentences = []\n",
        "    for sentence_predictions, sentence_scores, sentence in zip(predictions, scores, sentences):\n",
        "        output_sentence = []\n",
        "        for i, (top_predictions, top_scores) in enumerate(zip(sentence_predictions, sentence_scores)):\n",
        "            #print(sentence[i] +\": \",list(zip(top_predictions, top_scores)))    # debug\n",
        "            word = sentence[i]\n",
        "            output_sentence.append(annotate_word(word, top_predictions, top_scores, args, stopwords))\n",
        "        output_sentence = unify_links(output_sentence, sentence)\n",
        "        output_sentences.append(\" \".join(output_sentence))\n",
        "    #print(\" \".join(output_sentences))\n",
        "    return \"\"\"<h4>\"\"\" + \" \".join(output_sentences) + \"\"\"</h4>\"\"\"\n",
        "\n",
        "@contextmanager\n",
        "def suppress_stderr():\n",
        "    \"\"\"A context manager that redirects stdout and stderr to devnull\"\"\"\n",
        "    with open(os.devnull, 'w') as fnull:\n",
        "        with redirect_stderr(fnull) as err:\n",
        "            yield (err)\n",
        "\n",
        "def annotate_word(word, top_predictions, top_scores, args, stopwords):\n",
        "    ''' Choose a link from model's predictions according to the scores they achieved for that particular word. '''\n",
        "    if word in stopwords:\n",
        "        return (word,False)\n",
        "    link = None\n",
        "    for i in range(args.top_k):\n",
        "        if link is not None:\n",
        "            return (link,True)\n",
        "        if top_predictions[i] not in stopwords and top_scores[i] > args.perplexity:\n",
        "            link = get_wikilink(word, top_predictions[i])\n",
        "\n",
        "    # Below here we try to enhance model predictions by gathering information from all of the top_links\n",
        "    if top_scores[0] <= args.perplexity or top_predictions[0] in stopwords:\n",
        "        return (link,True) if link is not None else (word,False)\n",
        "\n",
        "    for i in range(1, args.top_k):\n",
        "        if link is not None:\n",
        "            return (link,True)\n",
        "        if mean([top_scores[0], top_scores[i]]) > args.perplexity:\n",
        "            # This time try concatenating couples of links which are considered enoguh confident\n",
        "            link = get_wikilink(word, top_predictions[0] + \" \" + top_predictions[i])\n",
        "    return (link,True) if link is not None else (word,False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4yH6UMJTSZ4",
        "colab_type": "text"
      },
      "source": [
        "###General utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARmh--sITGmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_state():\n",
        "    return {'train_loss' : [], 'train_acc' : [], 'val_loss' : [], 'val_acc' : [], 'test_loss' : -1, 'test_acc' : -1}\n",
        "\n",
        "def plot_results(train_state, save_path=None):\n",
        "    epochs = len(train_state['train_loss'])\n",
        "    plt.rcParams[\"figure.figsize\"] = (16,12)\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    loss_plot = plt.subplot(211)\n",
        "    loss_plot.set_xticks(np.arange(0, epochs, 1.0))\n",
        "    plt.title('model loss', fontsize=14)\n",
        "    plt.plot(train_state['train_loss'], label='train')\n",
        "    plt.plot(train_state['val_loss'], label='validation')\n",
        "    plt.ylabel('loss', fontsize=12)\n",
        "    plt.xlabel('epoch', fontsize=12)\n",
        "    plt.legend()\n",
        "    accuracy_plt = plt.subplot(212)\n",
        "    accuracy_plt.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "    accuracy_plt.set_xticks(np.arange(0, epochs, 1.0))\n",
        "    plt.title('model accuracy', fontsize=14)\n",
        "    plt.plot(train_state['train_acc'], label='train')\n",
        "    plt.plot(train_state['val_acc'], label='validation')\n",
        "    plt.ylabel('accuracy', fontsize=12)\n",
        "    plt.xlabel('epoch', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.subplots_adjust(hspace=0.4)\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def save_model(model, optimizer, train_state, vocabulary, args):\n",
        "    epochs = len(train_state['train_loss'])\n",
        "    state = {\n",
        "        #'epoch': epochs,\n",
        "        'state_dict': model.state_dict(),\n",
        "        #'optimizer': optimizer.state_dict(),\n",
        "        'vocabulary': vocabulary,\n",
        "    }\n",
        "    torch.save(state, os.path.join(args.save_dir + args.model_state_file))\n",
        "    train_state_path = os.path.join(args.save_dir, args.train_state_file)\n",
        "    plot_results(train_state, save_path=train_state_path+\".jpg\")\n",
        "    with open(train_state_path+\".json\", 'w') as f_out:\n",
        "        json.dump(train_state, f_out)\n",
        "    \n",
        "def format_link(text, link):\n",
        "    return \"\"\"<a href=\"{}\" target=\"_blank\">{}</a>\"\"\".format(link, text)\n",
        "\n",
        "def get_wikilink(word, link):\n",
        "    ''' This function is responsible for converting actual links (keywords) into real wikipedia links. '''\n",
        "    link = \"_\".join(link.split())\n",
        "    response = requests.request(method='HEAD', url=\"https://en.wikipedia.org/wiki/{}\".format(link.capitalize()))\n",
        "    if response.status_code == requests.codes.ok:\n",
        "        return response.url\n",
        "    try:\n",
        "        # Suppress errors form Wikipedia library as described here: https://github.com/goldsmith/Wikipedia/pull/112\n",
        "        with suppress_stderr():\n",
        "            return wikipedia.page(link.title()).url\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def load_file(path):\n",
        "    ''' Read a file of data separated by newlines. '''\n",
        "    with open(path, 'r') as f:\n",
        "        result_file = f.read().splitlines()\n",
        "    return result_file\n",
        "\n",
        "def unify_links(link_sequence, original_sentence):\n",
        "    ''' This is responsible for merging links which include more words in one sigle link, just to prettify. '''\n",
        "    output_sentence = []\n",
        "    link_tokens = []\n",
        "    for i, (token, is_link) in enumerate(link_sequence):\n",
        "        if is_link:\n",
        "            last_link, _ = link_sequence[i-1]\n",
        "            if len(link_tokens) > 0 and token != last_link:\n",
        "                # Check if it's another link\n",
        "                tokens = \" \".join(link_tokens)\n",
        "                link_tokens = []\n",
        "                output_sentence.append(format_link(tokens, last_link))\n",
        "\n",
        "            link_tokens.append(original_sentence[i])\n",
        "        else:\n",
        "            if len(link_tokens) > 0:\n",
        "                tokens = \" \".join(link_tokens)\n",
        "                link_tokens = []\n",
        "                last_link, _ = link_sequence[i-1]\n",
        "                output_sentence.append(format_link(tokens, last_link))\n",
        "            output_sentence.append(token)\n",
        "\n",
        "    if len(link_tokens) > 0:\n",
        "        tokens = \" \".join(link_tokens)\n",
        "        last_link, _ = link_sequence[-1]\n",
        "        output_sentence.append(format_link(tokens, last_link))\n",
        "    return output_sentence\n",
        "\n",
        "def store_test_sentences(dataset, path):\n",
        "    output_data = [\" \".join(sentence) for sentence in dataset.test_ds['source_sentences']]\n",
        "    with open(path, 'w') as f_out:\n",
        "        f_out.write(\"\\n\".join(output_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0Ef4IV2TawI",
        "colab_type": "text"
      },
      "source": [
        "###Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9ZVZE-J7cDB",
        "colab_type": "code",
        "outputId": "0dc17e6e-0e26-4c25-cdda-ff1a636aeca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "args = Namespace(\n",
        "    dataset_path=\"drive/My Drive/Wikifier/input_data/\",\n",
        "    max_files = -1,\n",
        "    dev=3,\n",
        "    save_dir=\"drive/My Drive/Wikifier/\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    train_state_file=\"model_performance\",\n",
        "    stopwords_file=\"stopwords.txt\",\n",
        "    test_file=\"sentences.txt\",\n",
        "    save_model=True,\n",
        "    embedding_dim = 256,\n",
        "    hidden_dim = 128,\n",
        "    learning_rate = 0.01,\n",
        "    weight_decay = 0.0001,\n",
        "    batch_size = 256,\n",
        "    perplexity=0.28,\n",
        "    top_k=3,\n",
        "    inference_batch_size = 1,\n",
        "    shuffle = True,\n",
        "    drop_last = True,\n",
        "    epochs = 16,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Running model using CPU\")\n",
        "    args.device = 'cpu'\n",
        "else:\n",
        "    print(\"Running model on GPU:\", torch.cuda.get_device_name(0), \"x\" + str(torch.cuda.device_count()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running model on GPU: Tesla P100-PCIE-16GB x1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg_asmyVThBM",
        "colab_type": "text"
      },
      "source": [
        "###Initializations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J4TVgKklQ5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = WikiDataset.load_from_files(args.dataset_path, max_files=args.max_files)\n",
        "dataset.reduce(dev=args.dev)\n",
        "store_test_sentences(dataset, path=args.save_dir+args.test_file)\n",
        "vocabulary = Vocabulary.of(dataset)\n",
        "dataset.encode_from(vocabulary)\n",
        "\n",
        "\n",
        "model = BiLSTM(vocab_size=vocabulary.source_size(), embedding_dim=args.embedding_dim, hidden_dim=args.hidden_dim, \\\n",
        "               target_size=vocabulary.target_size(), batch_size=args.batch_size, device=args.device).to(args.device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "train_state = make_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOA5s3mQT10l",
        "colab_type": "text"
      },
      "source": [
        "###Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02wXyTwqlQ5X",
        "colab_type": "code",
        "outputId": "75738792-b30c-4503-afc7-a7bb3a29ce72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "for epoch in range(args.epochs): \n",
        "\n",
        "    # Training\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=args.batch_size, shuffle=args.shuffle, drop_last=args.drop_last, collate_fn=partial(collate_fn, device=args.device))\n",
        "    batch_generator = iter(dataloader)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for batch_index in trange(len(batch_generator), leave=True, desc=\"Train progress\"):\n",
        "        batch = batch_generator.next()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, x_len = batch.values()\n",
        "        y_hat = model(x, x_len)\n",
        "\n",
        "        loss = criterion(y_hat, y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "        acc_t = compute_accuracy(y_hat, y)\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "    train_state['train_loss'].append(running_loss)\n",
        "    train_state['train_acc'].append(running_acc)\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    dataset.set_split('val')\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=args.batch_size, shuffle=args.shuffle, drop_last=args.drop_last, collate_fn=partial(collate_fn, device=args.device))\n",
        "    batch_generator = iter(dataloader)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    for batch_index in trange(len(batch_generator), leave=True, desc=\"Validation progress\"):\n",
        "        batch = batch_generator.next()\n",
        "\n",
        "        x, y, x_len = batch.values()\n",
        "        y_hat = model(x, x_len)\n",
        "\n",
        "        loss = criterion(y_hat, y.view(-1))\n",
        "\n",
        "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "        acc_t = compute_accuracy(y_hat, y)\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "    train_state['val_loss'].append(running_loss)\n",
        "    train_state['val_acc'].append(running_acc)\n",
        "\n",
        "# Test\n",
        "\n",
        "dataset.set_split('test')\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=args.batch_size, shuffle=args.shuffle, drop_last=args.drop_last, collate_fn=partial(collate_fn, device=args.device))\n",
        "batch_generator = iter(dataloader)\n",
        "\n",
        "running_loss = 0.0\n",
        "running_acc = 0.0\n",
        "model.eval()\n",
        "\n",
        "for batch_index in trange(len(batch_generator), leave=True, desc=\"Test\"):\n",
        "    batch = batch_generator.next()\n",
        "\n",
        "    x, y, x_len = batch.values()\n",
        "    y_hat = model(x, x_len)\n",
        "\n",
        "    loss = criterion(y_hat, y.view(-1))\n",
        "\n",
        "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "    acc_t = compute_accuracy(y_hat, y)\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train progress: 100%|██████████| 10064/10064 [33:25<00:00,  5.02it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:42<00:00, 17.73it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:13<00:00,  5.05it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:38<00:00, 18.16it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:14<00:00,  5.04it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:37<00:00, 18.23it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:09<00:00,  5.06it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:38<00:00, 18.15it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:18<00:00,  5.04it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:37<00:00, 18.24it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [32:59<00:00,  5.08it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:45<00:00, 17.40it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [32:58<00:00,  5.09it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:37<00:00, 18.30it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [32:52<00:00,  5.10it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:43<00:00, 17.62it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [32:58<00:00,  5.09it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:36<00:00, 18.40it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:07<00:00,  5.06it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:45<00:00, 17.38it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:01<00:00,  5.08it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:38<00:00, 18.11it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:08<00:00,  5.06it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:47<00:00, 17.18it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:12<00:00,  5.05it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:37<00:00, 18.25it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:11<00:00,  5.05it/s]\n",
            "Validation progress: 100%|██████████| 2876/2876 [02:46<00:00, 17.25it/s]\n",
            "Train progress: 100%|██████████| 10064/10064 [33:10<00:00,  5.06it/s]\n",
            "Validation progress:  98%|█████████▊| 2826/2876 [02:35<00:02, 18.09it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOLW2VmTWSu8",
        "colab_type": "text"
      },
      "source": [
        "###Visualize the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHjabSqcDTMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_results(train_state)\n",
        "if args.save_model:\n",
        "    save_model(model, optimizer, train_state, vocabulary, args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1WcG8wOWdoR",
        "colab_type": "text"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTm0mi9DqHWw",
        "colab_type": "text"
      },
      "source": [
        "###Initializations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtszEYwFEBfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = torch.load(args.save_dir + args.model_state_file)\n",
        "vocabulary = state['vocabulary']\n",
        "model = BiLSTM(vocab_size=vocabulary.source_size(), embedding_dim=args.embedding_dim, hidden_dim=args.hidden_dim, target_size=vocabulary.target_size(), batch_size=args.inference_batch_size, device=args.device).to(args.device)\n",
        "model.load_state_dict(state['state_dict'])\n",
        "#optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "#optimizer.load_state_dict(state['optimizer'])\n",
        "with open(args.save_dir + args.train_state_file + \".json\") as json_data:\n",
        "    train_state = json.load(json_data)\n",
        "\n",
        "model.eval()\n",
        "wikipedia.set_lang(\"en\")\n",
        "stopwords = load_file(os.path.join(args.save_dir, args.stopwords_file))\n",
        "test_sentences = load_file(os.path.join(args.save_dir, args.test_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc1UbafUqZof",
        "colab_type": "text"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDMKjba6qVeb",
        "colab_type": "code",
        "outputId": "a6233315-5320-48c8-969e-08e2b2751e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        }
      },
      "source": [
        "# Just an example sample sentence picked randomly from the test set\n",
        "sample_sentence = random.choice(test_sentences).capitalize()\n",
        "\n",
        "y = annotate(sample_sentence, stopwords, args)\n",
        "display(HTML(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h4>It is the most northerly <a href=\"https://en.wikipedia.org/wiki/Cathedral\" target=\"_blank\">cathedral</a> in the <a href=\"https://en.wikipedia.org/wiki/United_kingdom\" target=\"_blank\">united kingdom</a> , a fine example of <a href=\"https://en.wikipedia.org/wiki/Romanesque_architecture\" target=\"_blank\">romanesque architecture</a> built for the <a href=\"https://en.wikipedia.org/wiki/Bishop\" target=\"_blank\">bishops</a> of <a href=\"https://en.wikipedia.org/wiki/Orkney\" target=\"_blank\">orkney</a> when the <a href=\"https://en.wikipedia.org/wiki/Island\" target=\"_blank\">islands</a> were ruled by the norse earls of <a href=\"https://en.wikipedia.org/wiki/Orkney\" target=\"_blank\">orkney</a> .</h4>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku-FNn6idwYF",
        "colab_type": "text"
      },
      "source": [
        "##Future improvements and considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc9T7q3zot9R",
        "colab_type": "text"
      },
      "source": [
        "Since the model has been trained until a maximum of 15 epochs so far, it would be worth experiment longer training and maybe introduce early stopping to avoid overfitting on the dataset.\n",
        "\n",
        "Without doubts, the model would benefit from having real links instead of link keywords because sometimes the model correctly annotates an entity with the right link but then fails when converting the link to a valid URL.\n",
        "\n",
        "Currently, I don't have the chance to test the model using a larger input size but it would worth trying to incorporate morphological information about words being read when making predictions."
      ]
    }
  ]
}