{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import bz2\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "\n",
    "    test_slice = 0.15\n",
    "    val_slice = 0.15\n",
    "    inner_sep = '_'\n",
    "    outer_sep = '|'\n",
    "    link_cutoff = 3\n",
    "    _text_token = \"<TEXT>\"\n",
    "\n",
    "    def __init__(self, dataset, max_seq_len, strip_punctuation):\n",
    "        \n",
    "        self._dataset = dataset\n",
    "        self._max_seq_len = max_seq_len\n",
    "        self._strip_punctuation = strip_punctuation\n",
    "\n",
    "        self.train_ds = self._dataset['train']\n",
    "        self.train_size = len(self.train_ds['source_sentences'])\n",
    "\n",
    "        self.val_ds = self._dataset['val']\n",
    "        self.val_size = len(self.val_ds['source_sentences'])\n",
    "\n",
    "        self.test_ds = self._dataset['test']\n",
    "        self.test_size = len(self.test_ds['source_sentences'])\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_ds, self.train_size),\n",
    "                             'val': (self.val_ds, self.val_size),\n",
    "                             'test': (self.test_ds, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def is_a_link(cls, word):\n",
    "        return len(word) >= 2 and word[0] == cls.outer_sep and word[-1] == cls.outer_sep\n",
    "\n",
    "    @classmethod\n",
    "    def pre_process(cls, text):\n",
    "\n",
    "        # Text tokenizing\n",
    "        source_sentences = []\n",
    "        valid_links = []\n",
    "        for source_sentence in nltk.sent_tokenize(text):\n",
    "            tokens = nltk.word_tokenize(source_sentence)\n",
    "            for i in range(len(tokens)):\n",
    "                tokens[i] = tokens[i].lower()\n",
    "                if cls.is_a_link(tokens[i]):\n",
    "                    valid_links.append(tokens[i].split(cls.outer_sep)[-2])\n",
    "            source_sentences.append(tokens)\n",
    "\n",
    "        # Valid links is a set of the links which occur more than the treshold\n",
    "        valid_links = Counter(valid_links)\n",
    "        valid_links = set(link for link,frequence in valid_links.items() if frequence >= cls.link_cutoff)\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        max_seq_len = 0\n",
    "\n",
    "        # Form input and label sequences\n",
    "        for i, source_sentence in enumerate(source_sentences):\n",
    "            sentence = []\n",
    "            label = []\n",
    "            for j, word in enumerate(source_sentence):\n",
    "                if cls.is_a_link(word):\n",
    "                    _split = list(filter(None, word.split(cls.outer_sep)))\n",
    "                    if len(_split) == 2:\n",
    "                        text, link = _split\n",
    "                        sub_links = filter(None, text.split(cls.inner_sep))\n",
    "                        link = link.replace(\"_\", \" \") if link in valid_links else cls._text_token\n",
    "                        for sub_link in sub_links:\n",
    "                            label.append(link)\n",
    "                            sentence.append(sub_link)\n",
    "                    else:\n",
    "                        word = word.replace(cls.outer_sep, '').replace(cls.inner_sep, ' ')\n",
    "                        label.append(cls._text_token)\n",
    "                        sentence.append(word)\n",
    "                else:\n",
    "                    label.append(cls._text_token)\n",
    "                    sentence.append(word)\n",
    "            labels.append(label)\n",
    "            sentences.append(sentence)\n",
    "            max_seq_len = max(max_seq_len, len(sentence))\n",
    "\n",
    "        return sentences, labels, max_seq_len\n",
    "\n",
    "    @classmethod\n",
    "    def read_dataset(cls, ds_path):\n",
    "        \n",
    "        text = bz2.BZ2File(ds_path).read().decode('utf-8')\n",
    "        sentences, labels, max_seq_len = cls.pre_process(text)\n",
    "        train_size = int(len(sentences) * (1 - cls.test_slice - cls.val_slice))\n",
    "        test_size = int(len(sentences) * cls.test_slice)\n",
    "        return ({\n",
    "            'train': {'source_sentences': sentences[:train_size], 'target_labels' : labels[:train_size]},\n",
    "            'test': {'source_sentences': sentences[train_size:train_size+test_size], 'target_labels' : labels[train_size:train_size+test_size]},\n",
    "            'val': {'source_sentences' : sentences[train_size+test_size:], 'target_labels' : labels[train_size+test_size:]}\n",
    "        }, max_seq_len)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset(cls, ds_path, strip_punctuation=True):\n",
    "        \"\"\" Load dataset and make a new vectorizer from scratch \"\"\"\n",
    "        ds, max_seq_len = cls.read_dataset(ds_path)\n",
    "        return cls(ds, max_seq_len, strip_punctuation)\n",
    "\n",
    "    def encode_from(self, vocabulary):\n",
    "\n",
    "        for i, sentence in enumerate(self.train_ds['source_sentences']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.train_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.val_ds['source_sentences']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.val_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.test_ds['source_sentences']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.test_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.train_ds['target_labels']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.train_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.val_ds['target_labels']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.val_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.test_ds['target_labels']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.test_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
    "\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" Selects the splits in the dataset, from 'train', 'val' or 'test' \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_ds, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self._target_ds['source_sentences'][index]\n",
    "        links = self._target_ds['target_labels'][index]\n",
    "        #vector_dict = self._vectorizer.vectorize(sentence, links)\n",
    "        return {'x_source': sentence, 'y_target': links, 'x_source_length' : len(sentence)}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\"\"\"\n",
    "        return len(self) // batch_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    _padding_token = '<PAD>'\n",
    "    _unknown_token = '<UNK>'\n",
    "    \n",
    "    def __init__(self, word_to_index, link_to_index, text_token):\n",
    "        self._word_to_index = word_to_index\n",
    "        self._link_to_index = link_to_index\n",
    "        self._index_to_word = {i:w for i,w in enumerate(word_to_index)}\n",
    "        self._index_to_link = {i:l for i,l in enumerate(link_to_index)}\n",
    "        self.source_size = len(word_to_index)\n",
    "        self.target_size = len(link_to_index)\n",
    "        self._text_token = text_token\n",
    "    \n",
    "    def lookup_word(self, word):\n",
    "        return self._word_to_index.get(word, 1)\n",
    "\n",
    "    def lookup_link(self, link):\n",
    "        return self._link_to_index.get(link, 0)\n",
    "\n",
    "    def lookup_word_index(self, index):\n",
    "        return self._index_to_word.get(index, self._unknown_token)\n",
    "\n",
    "    def lookup_link_index(self, index):\n",
    "        return self._index_to_link.get(index, self._text_token)\n",
    "\n",
    "    @classmethod\n",
    "    def of(cls, ds):\n",
    "        source_vocab = dict()\n",
    "        target_vocab = dict()\n",
    "        text_token = ds._text_token\n",
    "        source_vocab = {cls._padding_token : 0, cls._unknown_token : 1}\n",
    "        target_vocab = {cls._padding_token : 0, text_token : 1}\n",
    "\n",
    "        for source_sequence in ds.train_ds['source_sentences']:\n",
    "            for token in source_sequence:\n",
    "                if token not in source_vocab:\n",
    "                    source_vocab[token] = len(source_vocab)\n",
    "\n",
    "        for target_sequence in ds.train_ds['target_labels']:\n",
    "            for token in target_sequence:\n",
    "                if token not in target_vocab:\n",
    "                    target_vocab[token] = len(target_vocab)\n",
    "\n",
    "        return cls(source_vocab, target_vocab, text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vectorize(indices, padding_index, vector_length):\n",
    "    vector = np.zeros(vector_length, dtype=np.int)\n",
    "    vector[:len(indices)] = indices\n",
    "    vector[len(indices):] = padding_index\n",
    "    return vector.tolist()\n",
    "\n",
    "def vectorize(input_sequence, padding_index=0, vector_length=-1):\n",
    "    if vector_length < 0:\n",
    "        vector_length = input_sequence['x_source_length']\n",
    "\n",
    "    source_sequence = _vectorize(input_sequence['x_source'], padding_index, vector_length)\n",
    "    target_sequence = _vectorize(input_sequence['y_target'], padding_index, vector_length)\n",
    "\n",
    "    return {'x_source' : source_sequence, 'y_target' : target_sequence, 'x_source_length' : input_sequence['x_source_length']}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda sample: sample['x_source_length'], reverse=True)\n",
    "    local_max_length = batch[0]['x_source_length']\n",
    "    batch = [vectorize(sequence, vector_length=local_max_length) for sequence in batch]\n",
    "    output_batch = {'x_source' : [], 'y_target' : [], 'x_source_length' : []}\n",
    "    for sample in batch:\n",
    "        output_batch['x_source'].append(sample['x_source'])\n",
    "        output_batch['y_target'].append(sample['y_target'])\n",
    "        output_batch['x_source_length'].append(sample['x_source_length'])\n",
    "    return {'x_source' : torch.LongTensor(output_batch['x_source']), 'y_target' : torch.LongTensor(output_batch['y_target']), 'x_source_length' : torch.LongTensor(output_batch['x_source_length'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, batch_size, num_layers=1, num_directions=2, padding_idx=0):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_directions, self.target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(self.num_layers * self.num_directions, self.batch_size, self.hidden_dim), \\\n",
    "        torch.randn(self.num_layers * self.num_directions, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, sequences, lengths):\n",
    "        # Reset LSTM hidden state, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        # Dim transformation: (batch_size, seq_size, 1) -> (batch_size, seq_size, embedding_dim)\n",
    "        embeds = self.word_embedding(sequences)\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        # Please note that output_lengths are the original 'lengths'\n",
    "        lstm_out, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        batch_size, seq_size, feat_size = lstm_out.shape\n",
    "\n",
    "        # Dim transformation: (batch_size, seq_size, hidden_size * directions) -> (batch_size * seq_size, hidden_size * directions)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        link_outputs = self.fc(lstm_out)\n",
    "        link_scores = F.log_softmax(link_outputs, dim=1)\n",
    "        \n",
    "        # Dim transformation: (batch_size * seq_size, target_size) -> (batch_size, seq_size, target_size)\n",
    "        link_scores = link_scores.view(batch_size, seq_size, self.target_size)\n",
    "        return link_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset.load_dataset(\"../input_data/wiki.txt.bz2\")\n",
    "vocabulary = Vocabulary.of(dataset)\n",
    "dataset.encode_from(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "28726.0\n"
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "hidden_dim = 64\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "device = 'cpu'\n",
    "learning_rate = 5e-4\n",
    "print(len(dataset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f420752f4319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_source_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8258b2128d62>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequences, lengths)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mlink_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mlink_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn, num_workers=4)\n",
    "model = BiLSTM(vocab_size=vocabulary.source_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, target_size=vocabulary.target_size, \n",
    "batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, x_len = batch['x_source'], batch['x_source_length']\n",
    "    y_hat = model(x, x_len)\n",
    "\n",
    "    if (i%100 == 0):\n",
    "        print(i)\n",
    "    \n",
    "\n",
    "#print(y_hat.shape)\n",
    "#_, y_hat = torch.max(y_hat, 2)\n",
    "#y_hat = torch.argmax(y_hat, 2)\n",
    "#y_true = batch['y_target'][:batch_size]\n",
    "\n",
    "#nn.functional.nll_loss(y_hat, y_true, ignore_index=0)\n",
    "#criterion(y_hat.float(), y_true.float())\n",
    "#F.cross_entropy(y_hat.float(), y_true.float(), ignore_index=0)\n",
    "#loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter())"
   ]
  }
 ]
}