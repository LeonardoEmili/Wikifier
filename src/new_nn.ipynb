{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import bz2\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "\n",
    "    test_slice = 0.15\n",
    "    val_slice = 0.15\n",
    "    inner_sep = '_'\n",
    "    outer_sep = '|'\n",
    "    link_cutoff = 3\n",
    "    _text_token = \"<TEXT>\"\n",
    "\n",
    "    def __init__(self, dataset, max_seq_len, strip_punctuation):\n",
    "        \n",
    "        self._dataset = dataset\n",
    "        self._max_seq_len = max_seq_len\n",
    "        self._strip_punctuation = strip_punctuation\n",
    "\n",
    "        self.train_ds = self._dataset['train']\n",
    "        self.train_size = len(self.train_ds['source_sentences'])\n",
    "\n",
    "        self.val_ds = self._dataset['val']\n",
    "        self.val_size = len(self.val_ds['source_sentences'])\n",
    "\n",
    "        self.test_ds = self._dataset['test']\n",
    "        self.test_size = len(self.test_ds['source_sentences'])\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_ds, self.train_size),\n",
    "                             'val': (self.val_ds, self.val_size),\n",
    "                             'test': (self.test_ds, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def is_a_link(cls, word):\n",
    "        return len(word) >= 2 and word[0] == cls.outer_sep and word[-1] == cls.outer_sep\n",
    "\n",
    "    @classmethod\n",
    "    def pre_process(cls, text):\n",
    "\n",
    "        # Text tokenizing\n",
    "        source_sentences = []\n",
    "        valid_links = []\n",
    "        for source_sentence in nltk.sent_tokenize(text):\n",
    "            tokens = nltk.word_tokenize(source_sentence)\n",
    "            for i in range(len(tokens)):\n",
    "                tokens[i] = tokens[i].lower()\n",
    "                if cls.is_a_link(tokens[i]):\n",
    "                    valid_links.append(tokens[i].split(cls.outer_sep)[-2])\n",
    "            source_sentences.append(tokens)\n",
    "\n",
    "        # Valid links is a set of the links which occur more than the treshold\n",
    "        valid_links = Counter(valid_links)\n",
    "        valid_links = set(link for link,frequence in valid_links.items() if frequence >= cls.link_cutoff)\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        max_seq_len = 0\n",
    "\n",
    "        # Form input and label sequences\n",
    "        for i, source_sentence in enumerate(source_sentences):\n",
    "            sentence = []\n",
    "            label = []\n",
    "            for j, word in enumerate(source_sentence):\n",
    "                if cls.is_a_link(word):\n",
    "                    _split = list(filter(None, word.split(cls.outer_sep)))\n",
    "                    if len(_split) == 2:\n",
    "                        text, link = _split\n",
    "                        sub_links = filter(None, text.split(cls.inner_sep))\n",
    "                        link = link.replace(\"_\", \" \") if link in valid_links else cls._text_token\n",
    "                        for sub_link in sub_links:\n",
    "                            label.append(link)\n",
    "                            sentence.append(sub_link)\n",
    "                    else:\n",
    "                        word = word.replace(cls.outer_sep, '').replace(cls.inner_sep, ' ')\n",
    "                        label.append(cls._text_token)\n",
    "                        sentence.append(word)\n",
    "                else:\n",
    "                    label.append(cls._text_token)\n",
    "                    sentence.append(word)\n",
    "            labels.append(label)\n",
    "            sentences.append(sentence)\n",
    "            max_seq_len = max(max_seq_len, len(sentence))\n",
    "\n",
    "        return sentences, labels, max_seq_len\n",
    "\n",
    "    @classmethod\n",
    "    def read_dataset(cls, ds_path):\n",
    "        \n",
    "        text = bz2.BZ2File(ds_path).read().decode('utf-8')\n",
    "        sentences, labels, max_seq_len = cls.pre_process(text)\n",
    "        train_size = int(len(sentences) * (1 - cls.test_slice - cls.val_slice))\n",
    "        test_size = int(len(sentences) * cls.test_slice)\n",
    "        return ({\n",
    "            'train': {'source_sentences': sentences[:train_size], 'target_labels' : labels[:train_size]},\n",
    "            'test': {'source_sentences': sentences[train_size:train_size+test_size], 'target_labels' : labels[train_size:train_size+test_size]},\n",
    "            'val': {'source_sentences' : sentences[train_size+test_size:], 'target_labels' : labels[train_size+test_size:]}\n",
    "        }, max_seq_len)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset(cls, ds_path, strip_punctuation=True):\n",
    "        \"\"\" Load dataset and make a new vectorizer from scratch \"\"\"\n",
    "        ds, max_seq_len = cls.read_dataset(ds_path)\n",
    "        return cls(ds, max_seq_len, strip_punctuation)\n",
    "\n",
    "    def encode_from(self, vocabulary):\n",
    "\n",
    "        for i, sentence in enumerate(self.train_ds['source_sentences']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.train_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.val_ds['source_sentences']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.val_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.test_ds['source_sentences']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.test_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.train_ds['target_labels']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.train_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.val_ds['target_labels']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.val_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
    "\n",
    "        for i, sentence in enumerate(self.test_ds['target_labels']):\n",
    "            for j, token in enumerate(sentence):\n",
    "                self.test_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
    "\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" Selects the splits in the dataset, from 'train', 'val' or 'test' \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_ds, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self._target_ds['source_sentences'][index]\n",
    "        links = self._target_ds['target_labels'][index]\n",
    "        #vector_dict = self._vectorizer.vectorize(sentence, links)\n",
    "        return {'x_source': sentence, 'y_target': links, 'x_source_length' : len(sentence)}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\"\"\"\n",
    "        return len(self) // batch_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    _padding_token = '<PAD>'\n",
    "    _unknown_token = '<UNK>'\n",
    "    \n",
    "    def __init__(self, word_to_index, link_to_index, text_token):\n",
    "        self._word_to_index = word_to_index\n",
    "        self._link_to_index = link_to_index\n",
    "        self._index_to_word = {i:w for i,w in enumerate(word_to_index)}\n",
    "        self._index_to_link = {i:l for i,l in enumerate(link_to_index)}\n",
    "        self.source_size = len(word_to_index)\n",
    "        self.target_size = len(link_to_index)\n",
    "        self._text_token = text_token\n",
    "    \n",
    "    def lookup_word(self, word):\n",
    "        return self._word_to_index.get(word, 1)\n",
    "\n",
    "    def lookup_link(self, link):\n",
    "        return self._link_to_index.get(link, 0)\n",
    "\n",
    "    def lookup_word_index(self, index):\n",
    "        return self._index_to_word.get(index, self._unknown_token)\n",
    "\n",
    "    def lookup_link_index(self, index):\n",
    "        return self._index_to_link.get(index, self._text_token)\n",
    "\n",
    "    @classmethod\n",
    "    def of(cls, ds):\n",
    "        source_vocab = dict()\n",
    "        target_vocab = dict()\n",
    "        text_token = ds._text_token\n",
    "        source_vocab = {cls._padding_token : 0, cls._unknown_token : 1}\n",
    "        target_vocab = {cls._padding_token : 0, text_token : 1}\n",
    "\n",
    "        for source_sequence in ds.train_ds['source_sentences']:\n",
    "            for token in source_sequence:\n",
    "                if token not in source_vocab:\n",
    "                    source_vocab[token] = len(source_vocab)\n",
    "\n",
    "        for target_sequence in ds.train_ds['target_labels']:\n",
    "            for token in target_sequence:\n",
    "                if token not in target_vocab:\n",
    "                    target_vocab[token] = len(target_vocab)\n",
    "\n",
    "        return cls(source_vocab, target_vocab, text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vectorize(indices, padding_index, vector_length):\n",
    "    vector = np.zeros(vector_length, dtype=np.int)\n",
    "    vector[:len(indices)] = indices\n",
    "    vector[len(indices):] = padding_index\n",
    "    return vector.tolist()\n",
    "\n",
    "def vectorize(input_sequence, padding_index=0, vector_length=-1):\n",
    "    if vector_length < 0:\n",
    "        vector_length = input_sequence['x_source_length']\n",
    "\n",
    "    source_sequence = _vectorize(input_sequence['x_source'], padding_index, vector_length)\n",
    "    target_sequence = _vectorize(input_sequence['y_target'], padding_index, vector_length)\n",
    "\n",
    "    return {'x_source' : source_sequence, 'y_target' : target_sequence, 'x_source_length' : input_sequence['x_source_length']}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda sample: sample['x_source_length'], reverse=True)\n",
    "    local_max_length = batch[0]['x_source_length']\n",
    "    batch = [vectorize(sequence, vector_length=local_max_length) for sequence in batch]\n",
    "    output_batch = {'x_source' : [], 'y_target' : [], 'x_source_length' : []}\n",
    "    for sample in batch:\n",
    "        output_batch['x_source'].append(sample['x_source'])\n",
    "        output_batch['y_target'].append(sample['y_target'])\n",
    "        output_batch['x_source_length'].append(sample['x_source_length'])\n",
    "    return {'x_source' : torch.LongTensor(output_batch['x_source']), 'y_target' : torch.LongTensor(output_batch['y_target']), 'x_source_length' : torch.LongTensor(output_batch['x_source_length'])}\n",
    "\n",
    "def compute_accuracy(y_hat, y, mask_index=0):\n",
    "    y_hat = torch.argmax(y_hat, dim=1)\n",
    "    y = y.view(-1)\n",
    "    correct_indices = torch.eq(y_hat, y).float()\n",
    "    valid_indices = torch.ne(y, mask_index).float()\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def make_state():\n",
    "    return {'train_loss' : [], 'train_acc' : [], 'val_loss' : [], 'val_acc' : [], 'test_loss' : -1, 'test_acc' : -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, batch_size, num_layers=1, num_directions=2, padding_idx=0):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_directions, self.target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(self.num_layers * self.num_directions, self.batch_size, self.hidden_dim), \\\n",
    "        torch.randn(self.num_layers * self.num_directions, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, sequences, lengths):\n",
    "        # Reset LSTM hidden state, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        # Dim transformation: (batch_size, seq_size, 1) -> (batch_size, seq_size, embedding_dim)\n",
    "        embeds = self.word_embedding(sequences)\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        # Please note that output_lengths are the original 'lengths'\n",
    "        lstm_out, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        batch_size, seq_size, feat_size = lstm_out.shape\n",
    "\n",
    "        # Dim transformation: (batch_size, seq_size, hidden_size * directions) -> (batch_size * seq_size, hidden_size * directions)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        link_outputs = self.fc(lstm_out)\n",
    "        #link_scores = F.log_softmax(link_outputs, dim=1)\n",
    "        # Output has the shape (batch_size * seq_size, target_size)\n",
    "        \n",
    "        return link_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset.load_dataset(\"../input_data/wiki.txt.bz2\")\n",
    "vocabulary = Vocabulary.of(dataset)\n",
    "dataset.encode_from(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_dim = 64\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "device = 'cpu'\n",
    "learning_rate = 5e-4\n",
    "print(len(dataset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(vocab_size=vocabulary.source_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, target_size=vocabulary.target_size, \n",
    "batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "state = make_state()\n",
    "\n",
    "for i in range(epochs):\n",
    "    dataset.set_split('train')\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_index, batch in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, x_len = batch.values()\n",
    "        y_hat = model(x, x_len)\n",
    "\n",
    "        loss = criterion(y_hat, y.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_hat, y)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BiLSTM(vocab_size=vocabulary.source_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, target_size=vocabulary.target_size, batch_size=batch_size)\n",
    "#x, y, x_len = batch.values()\n",
    "#y_hat = model(x, x_len)\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "#print(y_hat.shape, y.shape, x_len)\n",
    "#loss = criterion(y_hat, y.view(-1))\n",
    "#loss.backward()\n",
    ""
   ]
  }
 ]
}