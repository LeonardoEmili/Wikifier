{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "2.7.17-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "new_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMAhu2EIlQ40",
        "colab_type": "code",
        "outputId": "47d24a99-e9a2-4d58-ad82-14cd5b94ea70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import bz2\n",
        "import nltk\n",
        "from glob import glob\n",
        "from tqdm import trange\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from functools import partial\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Tesla P100-PCIE-16GB\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idle0UE1lQ48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiDataset(Dataset):\n",
        "\n",
        "    test_slice = 0.10\n",
        "    val_slice = 0.20\n",
        "    inner_sep = '_'\n",
        "    outer_sep = '|'\n",
        "    link_cutoff = 20\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        \n",
        "        self._dataset = dataset\n",
        "\n",
        "        self.train_ds = self._dataset['train']\n",
        "        self.val_ds = self._dataset['val']\n",
        "        self.test_ds = self._dataset['test']\n",
        "\n",
        "        self._lookup_dict = {'train': self.train_ds, 'val': self.val_ds, 'test': self.test_ds}\n",
        "        self.set_split('train')\n",
        "\n",
        "    def train_size(self):\n",
        "        return len(self.train_ds['source_sentences'])\n",
        "\n",
        "    def val_size(self):\n",
        "        return len(self.val_ds['source_sentences'])\n",
        "\n",
        "    def test_size(self):\n",
        "        return len(self.test_ds['source_sentences'])\n",
        "\n",
        "    @classmethod\n",
        "    def is_a_link(cls, word):\n",
        "        return len(word) >= 2 and word[0] == cls.outer_sep and word[-1] == cls.outer_sep\n",
        "\n",
        "    @classmethod\n",
        "    def pre_process(cls, text, strip_punctuation):\n",
        "        #punctuation = \"{[]()},.;:!?~^'\\\"\"\n",
        "        \n",
        "        source_sentences = [[token.lower() for token in nltk.word_tokenize(source_sentence)] for source_sentence in text.readlines()]\n",
        "        valid_links = Counter(token.split(cls.outer_sep)[-2] for source_sentence in source_sentences for token in source_sentence if cls.is_a_link(token))\n",
        "        valid_links = set(link for link,frequence in Counter(valid_links).items() if frequence >= cls.link_cutoff)\n",
        "        sentences = []\n",
        "        labels = []\n",
        "\n",
        "        for source_sentence in source_sentences:\n",
        "            sentence = []\n",
        "            label = []\n",
        "            has_valid_links = False\n",
        "            for token in source_sentence:\n",
        "                if cls.is_a_link(token):\n",
        "                    _split = list(filter(None, token.split(cls.outer_sep)))\n",
        "                    if len(_split) == 2:\n",
        "                        text, link = _split\n",
        "                        if link in valid_links:\n",
        "                            link = link.replace(\"_\", \" \")\n",
        "                            has_valid_links = True\n",
        "                        else:\n",
        "                            link = None\n",
        "                        sub_links = filter(None, text.split(cls.inner_sep))\n",
        "                        for sub_link in sub_links:\n",
        "                            label.append(link)\n",
        "                            sentence.append(sub_link)\n",
        "                    else:\n",
        "                        token = token.replace(cls.outer_sep, ' ').replace(cls.inner_sep, ' ')\n",
        "                        label.append(None)\n",
        "                        sentence.append(token)\n",
        "                else:\n",
        "                    label.append(None)\n",
        "                    sentence.append(token)\n",
        "            if has_valid_links:\n",
        "                labels.append(label)\n",
        "                sentences.append(sentence)\n",
        "        return sentences, labels\n",
        "\n",
        "    @classmethod\n",
        "    def read_dataset(cls, ds_path, strip_punctuation):\n",
        "        text = bz2.open(ds_path, mode='rt', encoding='utf-8')\n",
        "        sentences, labels = cls.pre_process(text, strip_punctuation)\n",
        "        train_size = int(len(sentences) * (1 - cls.test_slice - cls.val_slice))\n",
        "        test_size = int(len(sentences) * cls.test_slice)\n",
        "        return {\n",
        "            'train': {'source_sentences': sentences[:train_size], 'target_labels' : labels[:train_size]},\n",
        "            'test': {'source_sentences': sentences[train_size:train_size+test_size], 'target_labels' : labels[train_size:train_size+test_size]},\n",
        "            'val': {'source_sentences' : sentences[train_size+test_size:], 'target_labels' : labels[train_size+test_size:]}\n",
        "        }\n",
        "    \n",
        "    @classmethod\n",
        "    def load_from_files(cls, ds_dir, max_files=-1, file_ext=\"bz2\", strip_punctuation=False):\n",
        "        ds_dir = ds_dir + \"/\" if not ds_dir.endswith(\"/\") else ds_dir\n",
        "        dumps = glob(ds_dir + \"*.\" + file_ext)\n",
        "        max_files = len(dumps) if max_files <= -1 else max_files\n",
        "        ds = {\n",
        "            'train': {'source_sentences': [], 'target_labels' : []},\n",
        "            'test': {'source_sentences': [], 'target_labels' : []},\n",
        "            'val': {'source_sentences': [], 'target_labels' : []}\n",
        "        }\n",
        "        for i in range(min(max(1, max_files), len(dumps))):\n",
        "            path = dumps[i]\n",
        "            dump = cls.read_dataset(path, strip_punctuation)\n",
        "            for split in ds.keys():\n",
        "                ds[split]['source_sentences'] += dump[split]['source_sentences']\n",
        "                ds[split]['target_labels'] += dump[split]['target_labels']\n",
        "        return cls(ds)\n",
        "        \n",
        "\n",
        "    def reduce(self, dev):\n",
        "        train_mean = sum([len(x) for x in self.train_ds['source_sentences']]) / self.train_size()\n",
        "        val_mean = sum([len(x) for x in self.val_ds['source_sentences']]) / self.val_size()\n",
        "        test_mean = sum([len(x) for x in self.test_ds['source_sentences']]) / self.test_size()\n",
        "\n",
        "        self.train_ds['source_sentences'] = [x for x in self.train_ds['source_sentences'] if len(x) <= train_mean*dev]\n",
        "        self.train_ds['target_labels'] = [y for y in self.train_ds['target_labels'] if len(y) <= train_mean*dev]\n",
        "\n",
        "        self.val_ds['source_sentences'] = [x for x in self.val_ds['source_sentences'] if len(x) <= val_mean*dev]\n",
        "        self.val_ds['target_labels'] = [y for y in self.val_ds['target_labels'] if len(y) <= val_mean*dev]\n",
        "\n",
        "        self.test_ds['source_sentences'] = [x for x in self.test_ds['source_sentences'] if len(x) <= test_mean*dev]\n",
        "        self.test_ds['target_labels'] = [y for y in self.test_ds['target_labels'] if len(y) <= test_mean*dev]\n",
        "\n",
        "    def encode_from(self, vocabulary):\n",
        "\n",
        "        for i, sentence in enumerate(self.train_ds['source_sentences']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.train_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.val_ds['source_sentences']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.val_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.test_ds['source_sentences']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.test_ds['source_sentences'][i][j] = vocabulary.lookup_word(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.train_ds['target_labels']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.train_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.val_ds['target_labels']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.val_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
        "\n",
        "        for i, sentence in enumerate(self.test_ds['target_labels']):\n",
        "            for j, token in enumerate(sentence):\n",
        "                self.test_ds['target_labels'][i][j] = vocabulary.lookup_link(token)\n",
        "\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\" Selects the splits in the dataset, from 'train', 'val' or 'test' \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_ds = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._target_ds['source_sentences'])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self._target_ds['source_sentences'][index]\n",
        "        links = self._target_ds['target_labels'][index]\n",
        "        return {'x_source': sentence, 'y_target': links, 'x_source_length' : len(sentence)}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\"\"\"\n",
        "        return len(self) // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP6DCd2PlQ5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "\n",
        "    _padding_token = '<PAD>'\n",
        "    _unknown_token = '<UNK>'\n",
        "    \n",
        "    def __init__(self, word_to_index, link_to_index):\n",
        "        self._word_to_index = word_to_index\n",
        "        self._link_to_index = link_to_index\n",
        "        self._index_to_word = {i:w for i,w in enumerate(word_to_index)}\n",
        "        self._index_to_link = {i:l for i,l in enumerate(link_to_index)}\n",
        "\n",
        "    def source_size(self):\n",
        "        return len(self._word_to_index)\n",
        "\n",
        "    def target_size(self):\n",
        "        return len(self._link_to_index)\n",
        "    \n",
        "    def lookup_word(self, word):\n",
        "        return self._word_to_index.get(word, 1)\n",
        "\n",
        "    def lookup_link(self, link):\n",
        "        return self._link_to_index[link]\n",
        "\n",
        "    def lookup_word_index(self, index):\n",
        "        return self._index_to_word.get(index, self._unknown_token)\n",
        "\n",
        "    def lookup_link_index(self, index):\n",
        "        return self._index_to_link[index]\n",
        "\n",
        "    @classmethod\n",
        "    def of(cls, ds):\n",
        "        source_vocab = dict()\n",
        "        target_vocab = dict()\n",
        "        source_vocab = {cls._padding_token : 0, cls._unknown_token : 1}\n",
        "        target_vocab = {None : 0}\n",
        "\n",
        "        # Add words to the Vocabulary from the training set only\n",
        "        for source_sequence in ds.train_ds['source_sentences']:\n",
        "            for token in source_sequence:\n",
        "                if token not in source_vocab:\n",
        "                    source_vocab[token] = len(source_vocab)\n",
        "\n",
        "        # Add links to the Vocabulary from train, validation and test set. In fact, \n",
        "        # this should not influence predictionsbut provides reliability to the model.\n",
        "        for target_sequence in ds.train_ds['target_labels']:\n",
        "            for token in target_sequence:\n",
        "                if token not in target_vocab:\n",
        "                    target_vocab[token] = len(target_vocab)\n",
        "\n",
        "        for target_sequence in ds.val_ds['target_labels']:\n",
        "            for token in target_sequence:\n",
        "                if token not in target_vocab:\n",
        "                    target_vocab[token] = len(target_vocab)\n",
        "\n",
        "        for target_sequence in ds.test_ds['target_labels']:\n",
        "            for token in target_sequence:\n",
        "                if token not in target_vocab:\n",
        "                    target_vocab[token] = len(target_vocab)\n",
        "\n",
        "        return cls(source_vocab, target_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcEctpbdlQ5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _vectorize(indices, padding_index, vector_length):\n",
        "    vector = np.zeros(vector_length, dtype=np.int)\n",
        "    vector[:len(indices)] = indices\n",
        "    vector[len(indices):] = padding_index\n",
        "    return vector.tolist()\n",
        "\n",
        "def vectorize(input_sequence, padding_index=0, vector_length=-1):\n",
        "    if vector_length < 0:\n",
        "        vector_length = input_sequence['x_source_length']\n",
        "\n",
        "    source_sequence = _vectorize(input_sequence['x_source'], padding_index, vector_length)\n",
        "    target_sequence = _vectorize(input_sequence['y_target'], padding_index, vector_length)\n",
        "\n",
        "    return {'x_source' : source_sequence, 'y_target' : target_sequence, 'x_source_length' : input_sequence['x_source_length']}\n",
        "\n",
        "def collate_fn(batch, device='cpu'):\n",
        "    batch.sort(key=lambda sample: sample['x_source_length'], reverse=True)\n",
        "    local_max_length = batch[0]['x_source_length']\n",
        "    batch = [vectorize(sequence, vector_length=local_max_length) for sequence in batch]\n",
        "    output_batch = {'x_source' : [], 'y_target' : [], 'x_source_length' : []}\n",
        "    for sample in batch:\n",
        "        output_batch['x_source'].append(sample['x_source'])\n",
        "        output_batch['y_target'].append(sample['y_target'])\n",
        "        output_batch['x_source_length'].append(sample['x_source_length'])\n",
        "    return {'x_source' : torch.LongTensor(output_batch['x_source']).to(device), 'y_target' : torch.LongTensor(output_batch['y_target']).to(device), 'x_source_length' : torch.LongTensor(output_batch['x_source_length']).to(device)}\n",
        "\n",
        "def compute_accuracy(y_hat, y, mask_index=0):\n",
        "    y_hat = torch.argmax(y_hat, dim=1)\n",
        "    y = y.view(-1)\n",
        "    correct_indices = torch.eq(y_hat, y).float()\n",
        "    valid_indices = torch.ne(y, mask_index).float()\n",
        "    n_correct = (correct_indices * valid_indices).sum().item()\n",
        "    n_valid = max(1, valid_indices.sum().item())\n",
        "    return n_correct / n_valid * 100\n",
        "\n",
        "def make_state():\n",
        "    return {'train_loss' : [], 'train_acc' : [], 'val_loss' : [], 'val_acc' : [], 'test_loss' : -1, 'test_acc' : -1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgUMHf7HlQ5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, batch_size, num_layers=1, num_directions=2, padding_idx=0, device='cpu'):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.target_size = target_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_directions = num_directions\n",
        "        self.device = device\n",
        "\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * num_directions, self.target_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(self.num_layers * self.num_directions, self.batch_size, self.hidden_dim).to(self.device), \\\n",
        "        torch.randn(self.num_layers * self.num_directions, self.batch_size, self.hidden_dim).to(self.device))\n",
        "    \n",
        "    def forward(self, sequences, lengths):\n",
        "        # Reset LSTM hidden state, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "        # Dim transformation: (batch_size, seq_size, 1) -> (batch_size, seq_size, embedding_dim)\n",
        "        embeds = self.word_embedding(sequences)\n",
        "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, batch_first=True)\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        # Please note that output_lengths are the original 'lengths'\n",
        "        lstm_out, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        batch_size, seq_size, feat_size = lstm_out.shape\n",
        "\n",
        "        # Dim transformation: (batch_size, seq_size, hidden_size * directions) -> (batch_size * seq_size, hidden_size * directions)\n",
        "        lstm_out = lstm_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "        link_outputs = self.fc(lstm_out)\n",
        "        # Output has the shape (batch_size * seq_size, target_size)\n",
        "        \n",
        "        return link_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J4TVgKklQ5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_dir = \"drive/My Drive/Wikifier/input_data/\"\n",
        "dataset = WikiDataset.load_from_files(dataset_dir, max_files=3)\n",
        "dataset.reduce(dev=3)\n",
        "vocabulary = Vocabulary.of(dataset)\n",
        "dataset.encode_from(vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02wXyTwqlQ5X",
        "colab_type": "code",
        "outputId": "4edb2d45-66ce-41a5-cf1b-65a5610c083a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "embedding_dim = 128\n",
        "epochs = 6\n",
        "hidden_dim = 128\n",
        "batch_size = 32 # cpu\n",
        "device = 'cuda'\n",
        "learning_rate = 5e-4\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Running model using CPU\")\n",
        "    device = 'cpu'\n",
        "\n",
        "model = BiLSTM(vocab_size=vocabulary.source_size(), embedding_dim=embedding_dim, hidden_dim=hidden_dim, target_size=vocabulary.target_size(), \n",
        "batch_size=batch_size, device=device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "state = make_state()\n",
        "\n",
        "for epoch in range(epochs): \n",
        "\n",
        "    # Training\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=partial(collate_fn, device=device))\n",
        "    batch_generator = iter(dataloader)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for batch_index in trange(len(batch_generator), leave=True, desc=\"Train progress\"):\n",
        "        batch = batch_generator.next()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, x_len = batch.values()\n",
        "        y_hat = model(x, x_len)\n",
        "\n",
        "        loss = criterion(y_hat, y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "        acc_t = compute_accuracy(y_hat, y)\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "    state['train_loss'].append(running_loss)\n",
        "    state['train_acc'].append(running_acc)\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    dataset.set_split('val')\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=partial(collate_fn, device=device))\n",
        "    batch_generator = iter(dataloader)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    for batch_index in trange(len(batch_generator), leave=True, desc=\"Validation progress\"):\n",
        "        batch = batch_generator.next()\n",
        "\n",
        "        x, y, x_len = batch.values()\n",
        "        y_hat = model(x, x_len)\n",
        "\n",
        "        loss = criterion(y_hat, y.view(-1))\n",
        "\n",
        "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "        acc_t = compute_accuracy(y_hat, y)\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "    state['val_loss'].append(running_loss)\n",
        "    state['val_acc'].append(running_acc)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train progress: 100%|██████████| 38285/38285 [41:01<00:00, 15.55it/s]\n",
            "Validation progress: 100%|██████████| 10939/10939 [02:44<00:00, 66.37it/s]\n",
            "Train progress: 100%|██████████| 38285/38285 [41:29<00:00, 15.38it/s]\n",
            "Validation progress: 100%|██████████| 10939/10939 [02:45<00:00, 66.07it/s]\n",
            "Train progress: 100%|██████████| 38285/38285 [40:59<00:00, 15.57it/s]\n",
            "Validation progress: 100%|██████████| 10939/10939 [02:43<00:00, 66.73it/s]\n",
            "Train progress:  53%|█████▎    | 20265/38285 [21:51<18:05, 16.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsqQJdVyeoJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "0b687dc7-f1e9-4dc2-a1a2-9f28e19c1aa6"
      },
      "source": [
        "state"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_acc': -1,\n",
              " 'test_loss': -1,\n",
              " 'train_acc': [63.36483437339494,\n",
              "  67.53958216218163,\n",
              "  67.83562065113593,\n",
              "  67.95750506343197,\n",
              "  68.08966840854997,\n",
              "  68.20350568228632],\n",
              " 'train_loss': [2.9973329302861798,\n",
              "  2.5563685720717504,\n",
              "  2.525812911394156,\n",
              "  2.513982666662813,\n",
              "  2.5051425011425206,\n",
              "  2.4971641420072572],\n",
              " 'val_acc': [66.70500773492888,\n",
              "  67.07561542064825,\n",
              "  67.37145818049422,\n",
              "  67.7121195948738,\n",
              "  67.8176533194502,\n",
              "  67.82713073436844],\n",
              " 'val_loss': [2.6369154477742858,\n",
              "  2.5913013665492466,\n",
              "  2.574407027140208,\n",
              "  2.5676481639165214,\n",
              "  2.552776956364243,\n",
              "  2.5574564260901935]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAe5LIFe1XfW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfb43215-3b6c-4605-d3dc-a618a60095d6"
      },
      "source": [
        "dataset.set_split('test')\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=partial(collate_fn, device=device))\n",
        "batch_generator = iter(dataloader)\n",
        "\n",
        "running_loss = 0.0\n",
        "running_acc = 0.0\n",
        "model.eval()\n",
        "\n",
        "for batch_index in trange(len(batch_generator), leave=True, desc=\"Test\"):\n",
        "    batch = batch_generator.next()\n",
        "\n",
        "    x, y, x_len = batch.values()\n",
        "    y_hat = model(x, x_len)\n",
        "\n",
        "    loss = criterion(y_hat, y.view(-1))\n",
        "\n",
        "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "    acc_t = compute_accuracy(y_hat, y)\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "state['test_loss'] = running_loss\n",
        "state['test_acc'] = running_acc"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test: 100%|██████████| 5469/5469 [01:25<00:00, 64.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtszEYwFEBfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "72fd1278-4a70-4435-9596-5c7532a2d28f"
      },
      "source": [
        "#for k in state.items():\n",
        "    #print(k)\n",
        "dataset.set_split('train')\n",
        "y = 234\n",
        "print([vocabulary.lookup_word_index(k) for k in dataset[y]['x_source']])\n",
        "print([vocabulary.lookup_link_index(k) for k in dataset[y]['y_target']])\n",
        "indexes = [k for k in range(len(dataset[y]['y_target'])) if vocabulary.lookup_link_index(dataset[y]['y_target'][k]) != None]\n",
        "'''\n",
        "x = \"A preacher is a person who delivers sermons or homilies on religious topics to an assembly of people .\".lower()\n",
        "_l = len(x.split())\n",
        "x = [vocabulary.lookup_word(token) for token in x.split()]\n",
        "'''\n",
        "x = dataset[y]['x_source']\n",
        "_l = len(x)\n",
        "x = torch.LongTensor([_vectorize(x, 0, _l) for _ in range(batch_size)]).to(device)\n",
        "l = torch.LongTensor([_l for _ in range(batch_size)]).to(device)\n",
        "y_hat = model(x, l)\n",
        "y_hat = y_hat.view(batch_size, _l, -1)\n",
        "y_hat = torch.argmax(y_hat, dim=2)[0]\n",
        "y_hat = y_hat.tolist()\n",
        "p = [vocabulary.lookup_link_index(token) for token in y_hat]\n",
        "print([p[i] for i in indexes])\n",
        "#print(vocabulary.lookup_link('artificial intelligence'))\n",
        "print(p)\n",
        "#'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['it', 'flows', 'into', 'the', 'werse', 'in', 'münster', '.|_františek_čermák|františek_čermák|', 'and', 'pavel', 'vízner', 'were', 'the', 'defending', 'champions', ',', 'but', 'lost', 'in', 'the', 'semifinals', 'to', 'stéphane', 'bohli', 'and', 'stanislas', 'wawrinka', '.']\n",
            "[None, None, None, None, None, None, 'münster', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
            "['münster']\n",
            "['mookie betts', 'lava flow', 'chapters and verses of the bible', 'the undertaker', 'skipper', 'mentioned in despatches', 'münster', 'münster', 'mentioned in despatches', 'agnieszka radwańska', 'agnieszka radwańska', 'agnieszka radwańska', 'the undertaker', 'promotion and relegation', 'uefa champions league', 'promotion and relegation', 'promotion and relegation', 'lost film', 'run batted in', 'the undertaker', 'win', 'driving under the influence', 'driving under the influence', 'driving under the influence', 'run batted in', 'run batted in', 'run batted in', 'archery']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}