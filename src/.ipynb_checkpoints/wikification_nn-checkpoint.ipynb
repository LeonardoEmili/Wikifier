{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bz2\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612271\n",
      "129619\n"
     ]
    }
   ],
   "source": [
    "train_file = bz2.BZ2File('../input_data/train.json.bz2')\n",
    "test_file = bz2.BZ2File('../input_data/test.json.bz2')\n",
    "train_file = json.load(train_file)\n",
    "test_file = json.load(test_file)\n",
    "print(len(train_file))\n",
    "print(len(test_file))\n",
    "\n",
    "#train_file, train_links = process_input_file(train_file, valid_links, LINK_DELIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_file(input_file, valid_links, link_delim):\n",
    "    links = []\n",
    "    for i, line in enumerate(input_file):\n",
    "        sentence, link = next(iter(line.items()))\n",
    "        input_file[i] = sentence.lower().strip()\n",
    "        if link in valid_links:\n",
    "            input_file[i] = \"\".join([word.title() for word in input_file[i].split()])\n",
    "            links.append(link)\n",
    "    return \" \".join(input_file), links\n",
    "\n",
    "def encode_labels(text_with_link_markup, links, link2idx, link_delim, default_no_link):\n",
    "    encoded_labels = []\n",
    "    output_sentences = []\n",
    "    uppercase_regex = re.compile(\"[A-Z][^A-Z]*\")\n",
    "    link_index = 0\n",
    "    for sentence in nltk.sent_tokenize(text_with_link_markup):\n",
    "        encoded_label = []\n",
    "        output_sentence = []\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word[0].isupper():\n",
    "                sub_links = re.findall(uppercase_regex, word)\n",
    "                #sub_links = filter(None, word.split(link_delim))\n",
    "                for sub_link in sub_links:\n",
    "                    encoded_label.append(link2idx[links[link_index]])\n",
    "                    output_sentence.append(sub_link)\n",
    "                link_index += 1\n",
    "            else:\n",
    "                encoded_label.append(default_no_link)\n",
    "                output_sentence.append(word)\n",
    "        encoded_labels.append(encoded_label)\n",
    "        output_sentences.append(output_sentence)\n",
    "    return encoded_labels, output_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338198\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data ...\n",
    "\n",
    "LINK_DELIM = '_'\n",
    "link_treshold = 1\n",
    "valid_links = Counter([next(iter(sentence.values())) for sentence in train_file if next(iter(sentence.values())) is not None])\n",
    "valid_links = set([link for link,frequence in valid_links.items() if frequence >= link_treshold])\n",
    "\n",
    "train_file, train_links = process_input_file(train_file, valid_links, LINK_DELIM)\n",
    "test_file, test_links = process_input_file(test_file, valid_links, LINK_DELIM)\n",
    "\n",
    "output = train_links + [\"_TEXT\"]\n",
    "link2idx = {l:i for i,l in enumerate(output)}\n",
    "idx2link = {i:l for i,l in enumerate(output)}\n",
    "default_no_link = len(output) -1\n",
    "\n",
    "train_encoded_labels, train_sentences = encode_labels(train_file, train_links, link2idx, LINK_DELIM, default_no_link)\n",
    "test_encoded_labels, test_sentences = encode_labels(test_file, test_links, link2idx, LINK_DELIM, default_no_link)\n",
    "\n",
    "vocabulary = Counter([word for sentence in train_sentences for word in sentence])\n",
    "vocabulary = ['_PAD','_UNK'] + sorted(vocabulary, key=vocabulary.get, reverse=True)\n",
    "\n",
    "word2idx = {w:i for i,w in enumerate(vocabulary)}\n",
    "idx2word = {i:w for i,w in enumerate(vocabulary)} # probably vocabulary array is enough\n",
    "\n",
    "print(len(train_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(Physicist|Physicist)', '(Chemist|Chemist)', '(Electricity|Electricity)', '(Power|Power)', '(Electric|Electric Battery)', '(Battery|Electric Battery)', '(Methane|Methane)']\n",
      "['(Voltaic|Voltaic Pile)', '(Pile|Voltaic Pile)', '(Royal|Royal Society)', '(Society|Royal Society)']\n",
      "['(Electrochemistry|Electrochemistry)']\n",
      "['(Napoleon|Napoleon Bonaparte)', '(Bonaparte|Napoleon Bonaparte)', '(Institute|Institute Of France)', '(Of|Institute Of France)', '(France|Institute Of France)']\n",
      "['(University|University Of Pavia)', '(Of|University Of Pavia)', '(Pavia|University Of Pavia)']\n",
      "['(Si|International System Of Units)', '(Electric|Electric Potential)', '(Potential|Electric Potential)', '(Volt|Volt)']\n",
      "['(Como|Como)', '(Italy|Italy)']\n",
      "['(Electrophorus|Electrophorus)', '(Static|Static Electricity)', '(Electricity|Static Electricity)']\n",
      "['(Johan|Johan Wilcke)', '(Wilcke|Johan Wilcke)']\n",
      "['(H.|H. B. De Saussure)', '(B.|H. B. De Saussure)', '(De|H. B. De Saussure)', '(Saussure|H. B. De Saussure)']\n",
      "['(Chemistry|Chemistry)']\n",
      "['(Methane|Methane)', '(Benjamin|Benjamin Franklin)', '(Franklin|Benjamin Franklin)']\n",
      "['(Lake|Lake Maggiore)', '(Maggiore|Lake Maggiore)']\n",
      "['(Ignition|Combustion)', '(Spark|Electrostatic Discharge)']\n",
      "['(Capacitance|Capacitance)']\n",
      "['(Volt|Volt)']\n",
      "['(University|University Of Pavia)', '(Of|University Of Pavia)', '(Pavia|University Of Pavia)']\n",
      "['(Luigi|Luigi Galvani)', '(Galvani|Luigi Galvani)']\n",
      "['(Electric|Electric Current)', '(Current|Electric Current)']\n",
      "['(Electrochemical|Electrochemical Series)', '(Series|Electrochemical Series)', '(Electromotive|Electromotive Force)', '(Force|Electromotive Force)', '(Galvanic|Galvanic Cell)', '(Cell|Galvanic Cell)', '(Electrodes|Electrode)']\n",
      "['(Voltaic|Voltaic Pile)', '(Pile|Voltaic Pile)', '(Electric|Battery)', '(Battery|Battery)']\n",
      "['(Zinc|Zinc)', '(Copper|Copper)']\n",
      "['(Brine|Brine)']\n",
      "['(William|William Nicholson)', '(Nicholson|William Nicholson)', '(Tiberius|Tiberius Cavallo)', '(Cavallo|Tiberius Cavallo)', '(Abraham|Abraham Bennet)', '(Bennet|Abraham Bennet)']\n",
      "['(Zinc|Zinc)', '(Copper|Copper)']\n",
      "['(Electrolyte|Electrolyte)', '(Sulfuric|Sulfuric Acid)', '(Acid|Sulfuric Acid)', '(Brine|Brine)']\n",
      "['(Electrochemical|Standard Electrode Potential)', '(Series|Standard Electrode Potential)']\n",
      "['(Electrons|Electron)']\n",
      "['(Chemical|Chemical Reaction)', '(Reactions|Chemical Reaction)']\n",
      "['(Royal|Royal Netherlands Academy Of Arts And Sciences)', '(Institute|Royal Netherlands Academy Of Arts And Sciences)', '(Of|Royal Netherlands Academy Of Arts And Sciences)', '(The|Royal Netherlands Academy Of Arts And Sciences)', '(Netherlands|Royal Netherlands Academy Of Arts And Sciences)']\n",
      "['(Napoleon|Napoleon Bonaparte)', '(Bonaparte|Napoleon Bonaparte)', '(Frazione|Frazione)', '(Como|Como, Italy)', '(Italy|Tempio Voltiano)']\n",
      "['(Tempio|Villa Olmo)', '(Voltiano|Villa Olmo)']\n",
      "['(Villa|Italian Lira)', '(Olmo|Italian Lira)']\n",
      "['(Italian10,000|Volta)', '(Lire|Volta)', '(Note|Volta)']\n",
      "['(Volta|Pascal)', '(Pascal|Turing)', '(Turing|Pacifism)']\n",
      "['(Pacifist|Nuclear Weapon)']\n",
      "['(Nuclear|Anglican Pacifist Fellowship)', '(Weapons|Anglican Pacifist Fellowship)']\n",
      "['(Anglican|Central Committee For Conscientious Objectors)', '(Pacifist|Central Committee For Conscientious Objectors)', '(Fellowship|Central Committee For Conscientious Objectors)', '(Central|Christian Peacemaker Teams)', '(Committee|Christian Peacemaker Teams)', '(For|Christian Peacemaker Teams)', '(Conscientious|Christian Peacemaker Teams)', '(Objectors|Christian Peacemaker Teams)', '(Christian|Fellowship Of Reconciliation)', '(Peacemaker|Fellowship Of Reconciliation)', '(Teams|Fellowship Of Reconciliation)', '(Fellowship|Mennonites)', '(Of|Mennonites)', '(Reconciliation|Mennonites)', '(Mennonites|Peace Brigades International)', '(Peace|Peace Pledge Union)', '(Brigades|Peace Pledge Union)', '(International|Peace Pledge Union)', '(Peace|Religious Society Of Friends)', '(Pledge|Religious Society Of Friends)', '(Union|Religious Society Of Friends)', '(Religious|Soka University Of America)', '(Society|Soka University Of America)', '(Of|Soka University Of America)', '(Friends|Soka University Of America)', '(Soka|War Resisters International)', '(University|War Resisters International)', '(Of|War Resisters International)', '(America|War Resisters International)', '(War|War Resisters League)', '(Resisters|War Resisters League)', '(International|War Resisters League)', '(War|Cnd)', '(Resisters|Cnd)', '(League|Cnd)', '(Cnd|Pugwash Conferences On Science And World Affairs)', '(Pugwash|Greenpeace)', '(Greenpeace|Astronomy)', '(Astronomy|Star)', '(Stars|Color Index)', '(Color|Absolute Magnitude)', '(Brightness|Hertzsprung–Russell Diagram)']\n",
      "['(Hertzsprung–|Ejnar Hertzsprung)', '(Russell|Ejnar Hertzsprung)', '(Diagrams|Ejnar Hertzsprung)', '(Ejnar|Henry Norris Russell)', '(Hertzsprung|Henry Norris Russell)', '(Henry|Dwarf Star)', '(Norris|Dwarf Star)', '(Russell|Dwarf Star)']\n",
      "['(Dwarf|Sun)', '(Stars|Sun)']\n",
      "['(Sun|Thermal Energy)']\n",
      "['(Thermal|Stellar Core)', '(Energy|Stellar Core)', '(Core|Nuclear Fusion)', '(Region|Nuclear Fusion)', '(Nuclear|Hydrogen)', '(Fusion|Hydrogen)', '(Hydrogen|Helium)', '(Helium|Hydrostatic Equilibrium)']\n",
      "['(Hydrostatic|Gravitational Collapse)', '(Equilibrium|Gravitational Collapse)', '(Gravitational|Photosphere)', '(Collapse|Photosphere)']\n",
      "['(Photosphere|Radiation)']\n",
      "['(Radiation|Convection)', '(Convection|Solar Mass)']\n",
      "['(Mass|Proton–Proton Chain)', '(Of|Proton–Proton Chain)', '(The|Proton–Proton Chain)', '(Sun|Proton–Proton Chain)', '(Proton–|Carbon)', '(Proton|Carbon)', '(Chain|Carbon)']\n",
      "['(Carbon|Nitrogen)', '(Nitrogen|Oxygen)', '(Oxygen|Cno Cycle)', '(Cno|Stellar Evolution)', '(Cycle|Stellar Evolution)']\n",
      "['(Evolves|Supergiant)', '(Supergiant|Red Giant)', '(Red|White Dwarf)', '(Giant|White Dwarf)', '(White|Star)', '(Dwarf|Star)']\n",
      "['(Stars|Stellar Spectrum)']\n",
      "['(Spectra|Annie Jump Cannon)']\n",
      "['(Annie|Edward C. Pickering)', '(Jump|Edward C. Pickering)', '(Cannon|Edward C. Pickering)', '(Edward|Harvard College Observatory)', '(C.|Harvard College Observatory)', '(Pickering|Harvard College Observatory)', '(Harvard|Stellar Classification)', '(College|Stellar Classification)', '(Observatory|Stellar Classification)', '(Harvard|Potsdam)', '(Classification|Potsdam)', '(Scheme|Potsdam)', '(Potsdam|Ejnar Hertzsprung)', '(Ejnar|Star Cluster)', '(Hertzsprung|Star Cluster)']\n",
      "['(Star|Luminosity)', '(Clusters|Luminosity)']\n",
      "['(Luminosity|Princeton University)']\n",
      "['(Princeton|Henry Norris Russell)', '(University|Henry Norris Russell)', '(Henry|Absolute Magnitude)', '(Norris|Absolute Magnitude)', '(Russell|Absolute Magnitude)']\n",
      "['(Absolute|Parallax)', '(Magnitude|Parallax)']\n",
      "['(Parallax|Bengt Strömgren)']\n",
      "['(Bengt|Vogt–Russell Theorem)', '(Strömgren|Vogt–Russell Theorem)']\n",
      "['(Vogt–|Heinrich Vogt)', '(Russell|Heinrich Vogt)', '(Theorem|Heinrich Vogt)', '(Heinrich|Stellar Classification)', '(Vogt|Stellar Classification)']\n",
      "['(Stellar|William Wilson Morgan)', '(Classification|William Wilson Morgan)', '(William|Philip Childs Keenan)', '(Wilson|Philip Childs Keenan)', '(Morgan|Philip Childs Keenan)', '(Philip|Spectral Type)', '(Childs|Spectral Type)', '(Keenan|Spectral Type)']\n",
      "['(Spectral|Star)', '(Types|Star)']\n",
      "['(Star|Icarus)', '(Icarus|Earth)', '(Earth|Protostar)']\n",
      "['(Protostar|Jeans Instability)', '(Collapse|Giant Molecular Cloud)', '(Giant|Interstellar Medium)', '(Molecular|Interstellar Medium)', '(Cloud|Interstellar Medium)', '(Interstellar|Pre-Main-Sequence Star)', '(Medium|Pre-Main-Sequence Star)']\n",
      "['(Pre-|Exothermic)', '(Main-|Exothermic)', '(Sequence|Exothermic)', '(Star|Exothermic)']\n",
      "['(Exothermic|Nuclear Fusion)', '(Nuclear|Curve)', '(Fusion|Curve)']\n"
     ]
    }
   ],
   "source": [
    "x = max([x for sentence in train_encoded_labels for x in sentence])\n",
    "#print(x)\n",
    "for i, sentence in enumerate(train_sentences[:120]):\n",
    "    _ls = []\n",
    "    #print(train_encoded_labels[i])\n",
    "    for j, idx in enumerate(train_encoded_labels[i]):\n",
    "        if idx2link[idx] != \"_TEXT\":\n",
    "            _ls.append(\"({}|{})\".format(sentence[j],idx2link[idx]))\n",
    "    if (len(_ls)):\n",
    "        print(_ls)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#y = 0\n",
    "#print([_x for _x in [x for x in train_sentences][y]])\n",
    "#print([idx2link[_x] for _x in [x for x in train_encoded_labels][y]])\n",
    "#print(idx2link[x-1])\n",
    "#for sentence in train_encoded_labels[:50]:\n",
    "    #print([idx2link[x] for x in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode words as integers ...\n",
    "\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 1 for word in sentence]\n",
    "    \n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    test_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([idx2word[x] for x in train_sentences[0]])\n",
    "print([x for x in train_file[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('../input_data/train.json.bz2')\n",
    "train_file = json.load(train_file)\n",
    "#print(len(train_file))\n",
    "\n",
    "link_treshold = 1\n",
    "valid_links = Counter([next(iter(sentence.values())) for sentence in train_file if next(iter(sentence.values())) is not None])\n",
    "valid_links = set([link for link,frequence in valid_links.items() if frequence >= link_treshold])\n",
    "\n",
    "train_links = []\n",
    "for i, line in enumerate(train_file):\n",
    "    sentence, link = next(iter(line.items()))\n",
    "    train_file[i] = sentence.lower().strip()\n",
    "    if link is not None:\n",
    "        train_file[i] = \"\".join([word.title() for word in train_file[i].split(' ')])\n",
    "        train_links.append(link)\n",
    "\n",
    "output = train_links + [\"_TEXT\"]\n",
    "default_no_link = len(output) - 1\n",
    "\n",
    "train_labels = []\n",
    "train_sentences = []\n",
    "link_index = 0  \n",
    "for sentence in nltk.sent_tokenize(\" \".join(train_file)):\n",
    "    train_label = []\n",
    "    train_sentence = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        if word[0].isalpha() and word[0].isupper():\n",
    "            #sub_links = filter(None, word.split(LINK_DELIM))\n",
    "            sub_links = re.findall('[A-Z][^A-Z]*', word)\n",
    "            #try:\n",
    "            for sub_link in sub_links:\n",
    "                train_label.append(link_index)\n",
    "                train_sentence.append(sub_link)\n",
    "            link_index += 1\n",
    "            #except:\n",
    "             #   print(word)\n",
    "        else:\n",
    "            train_label.append(default_no_link)\n",
    "            train_sentence.append(word)\n",
    "    train_labels.append(train_label)\n",
    "    train_sentences.append(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', '1933', ',', 'Bengt', 'Strömgren', 'introduced', 'the', 'term', 'hertzsprung–russell', 'diagram', 'to', 'denote', 'a', 'luminosity-spectral', 'class', 'diagram', '.']\n",
      "[338198, 338198, 338198, 122, 122, 338198, 338198, 338198, 338198, 338198, 338198, 338198, 338198, 338198, 338198, 338198, 338198]\n"
     ]
    }
   ],
   "source": [
    "y = 104\n",
    "print(train_sentences[y])\n",
    "print(train_labels[y])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
